{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp importance_sampling\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling for Partially Gaussian State Space Models\n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=section.3.6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After having observed $Y$ one is usually interested in properties of the conditional distribution of states $X$ given $Y$. Typically this means terms of the form\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf E (f(X) | Y) &= \\mathbf E (f(X_0, \\dots, X_n) | Y_0, \\dots, Y_n) \\\\\n",
    "    &= \\int f(x_0, \\dots, x_n) p(x_0, \\dots, x_n | y_0, \\dots, y_n) \\mathrm d x_0 \\dots \\mathrm d x_n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As the density $p(x|y)$ is known only up to a constant, we resort to importance sampling with a GLSSM, represented by its gaussian densities $g$. The [Laplace approximation](30_laplace_approximation.ipynb) and [(modified) efficient importance sampling](50_modified_efficient_importance_sampling.ipynb) perform this task for loc concave state space models where the states are jointly gaussian. \n",
    "\n",
    "Both methods construct surrogate [linear gaussian state space models](00_glssm.ipynb) that are parameterized by synthetic observations $z_t$ and their covariance matrices $\\Omega_t$. Usually $\\Omega_t$ is a diagonal matrix which is justified if the components of the observation vector at time $t$, $Y^i_t$, $i = 1, \\dots, p$ are conditionally independent given states $X_t$.\n",
    "\n",
    "These models are then based on the following SSM:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X_0 &\\sim \\mathcal N (x_0, \\Sigma_0) &&\\\\\n",
    "    X_{t + 1} &= A_t X_{t} + \\varepsilon_{t + 1} &&t = 0, \\dots, n - 1\\\\\n",
    "    \\varepsilon_t &\\sim \\mathcal N (0, \\Sigma_t) && t = 1, \\dots, n \\\\\n",
    "    S_t &= B_t X_t &&\\\\\n",
    "    Z_t &= S_t + \\eta_t && t =0, \\dots, n & \\\\\n",
    "    \\eta_t &\\sim \\mathcal N(0, \\Omega_t) && t=0, \\dots, n.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting we can transform the expectation w.r.t the condtiional density $p(x|y)$ to one w.r.t the density $g(x|z)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int f(x) p(x|y) \\mathrm d x &= \\int f(x) \\frac{p(x|y)}{g(x|z)} g(x|z) \\mathrm d x\\\\\n",
    "&= \\int f(x) \\frac{p(y|x)}{g(z|x)} \\frac{g(z)}{p(y)} g(x|z) \\mathrm d x.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let $w(x) = \\frac{p(y|x)}{g(z|x)} = \\frac{p(y|s)}{g(z|s)}$ be the (unnormalized) importance sampling weights which only depend on $s_t = B_t x_t$, $t = 0, \\dots, n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tensorflow_probability.substrates.jax.distributions import (\n",
    "    MultivariateNormalFullCovariance as MVN,\n",
    ")\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Float, Array\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "from isssm.typing import PGSSM\n",
    "\n",
    "\n",
    "def log_weights_t(\n",
    "    s_t: Float[Array, \"p\"],  # signal\n",
    "    y_t: Float[Array, \"p\"],  # observation\n",
    "    xi_t: Float[Array, \"p\"],  # parameters\n",
    "    dist,  # observation distribution\n",
    "    z_t: Float[Array, \"p\"],  # synthetic observation\n",
    "    Omega_t: Float[Array, \"p p\"],  # synthetic observation covariance, assumed diagonal\n",
    ") -> Float:  # single log weight\n",
    "    \"\"\"Log weight for a single time point.\"\"\"\n",
    "    p_ys = dist(s_t, xi_t).log_prob(y_t).sum()\n",
    "\n",
    "    # omega_t = jnp.sqrt(jnp.diag(Omega_t))\n",
    "    # g_zs = MVN_diag(s_t, omega_t).log_prob(z_t).sum()\n",
    "    g_zs = MVN(s_t, Omega_t).log_prob(z_t).sum()\n",
    "\n",
    "    return p_ys - g_zs\n",
    "\n",
    "\n",
    "def log_weights(\n",
    "    s: Float[Array, \"n+1 p\"],  # signals\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    dist,  # observation distribution\n",
    "    xi: Float[Array, \"n+1 p\"],  # observation parameters\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # synthetic observation covariances:\n",
    ") -> Float:  # log weights\n",
    "    \"\"\"Log weights for all time points\"\"\"\n",
    "    p_ys = dist(s, xi).log_prob(y).sum()\n",
    "\n",
    "    # avoid triangular solve problems\n",
    "    # omega = jnp.sqrt(vmap(jnp.diag)(Omega))\n",
    "    # g_zs = MVN_diag(s, omega).log_prob(z).sum()\n",
    "    g_zs = MVN(s, Omega).log_prob(z).sum()\n",
    "\n",
    "    return p_ys - g_zs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance samples are generated from the smoothing distribution in the surrogate model, i.e. from  $g(x|z)$ using e.g. the [FFBS](10_kalman_filter_smoother.ipynb##FFBS) or [simulation smoother](./10_kalman_filter_smoother.ipynb#simulation-smoother) algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.pgssm import simulate_pgssm, nb_pgssm_running_example\n",
    "from isssm.laplace_approximation import laplace_approximation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jaxtyping import Float, Array, PRNGKeyArray\n",
    "from isssm.kalman import FFBS, simulation_smoother\n",
    "import jax.random as jrn\n",
    "from functools import partial\n",
    "from isssm.typing import GLSSM, PGSSM\n",
    "\n",
    "\n",
    "def pgssm_importance_sampling(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # model\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # covariance of synthetic observations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    ") -> tuple[\n",
    "    Float[Array, \"N n+1 m\"], Float[Array, \"N\"]\n",
    "]:  # importance samples and weights\n",
    "    u, A, D, Sigma0, Sigma, v, B, dist, xi = model\n",
    "    glssm = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    s = simulation_smoother(glssm, z, N, subkey)\n",
    "\n",
    "    model_log_weights = partial(log_weights, y=y, dist=dist, xi=xi, z=z, Omega=Omega)\n",
    "\n",
    "    lw = vmap(model_log_weights)(s)\n",
    "\n",
    "    return s, lw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform importance sampling for our [running example](20_lcssm.ipynb#running) using the model obtained by [the laplace approximation](30_laplace_approximation.ipynb). Notice that we obtain four times the number of samples that we specified, which comes from the use of [antithetics](./99_util.ipynb#antithetic-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jrn.PRNGKey(512)\n",
    "key, subkey = jrn.split(key)\n",
    "s_order = 4\n",
    "model = nb_pgssm_running_example(\n",
    "    n=100,\n",
    "    s_order=s_order,\n",
    "    Sigma0_seasonal=0.1 * jnp.eye(s_order - 1),\n",
    "    x0_seasonal=jnp.zeros(s_order - 1),\n",
    ")\n",
    "N = 1\n",
    "(x,), (y,) = simulate_pgssm(model, N, subkey)\n",
    "proposal, info = laplace_approximation(y, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(y)\n",
    "axs[1].plot(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "key, subkey = jrn.split(key)\n",
    "samples, lw = pgssm_importance_sampling(y, model, proposal.z, proposal.Omega, N, subkey)\n",
    "plt.scatter(jnp.arange(4 * N), lw)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights should be calculated on the log scale, but we need them on the usual scale to use for Monte-Carlo integration. These are called **auto-normalised weights** and defined by $W(X^i) = \\frac{w(X^i)}{\\sum_{i = 1}^N w(X^i)}$.\n",
    "\n",
    "As weights are only known up to a constant, we make exponentiation numerically stable by substracting (on the log-scale) the largest weight, ensuring that $\\log w^i \\leq 0$ for all weights and so $\\sum_{i = 1}^N w^i \\leq N$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jaxtyping import Float, Array\n",
    "\n",
    "\n",
    "def normalize_weights(\n",
    "    log_weights: Float[Array, \"N\"]  # log importance sampling weights\n",
    ") -> Float[Array, \"N\"]:  # normalized importance sampling weights\n",
    "    \"\"\"Normalize importance sampling weights.\"\"\"\n",
    "    max_weight = jnp.max(log_weights)\n",
    "\n",
    "    log_weights_corrected = log_weights - max_weight\n",
    "\n",
    "    weights = jnp.exp(log_weights_corrected)\n",
    "\n",
    "    return weights / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = normalize_weights(lw)\n",
    "plt.boxplot(weights[None] * N)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effective Sample Size\n",
    "\n",
    "The effective sample size is an important diagnostic for the performance of importance sampling, it is defined by\n",
    "\n",
    "$$\n",
    "\\text{ESS} = \\frac{\\left(\\sum_{i = 1}^{N} w(X^i)\\right)^2}{\\sum_{i = 1}^N w^2(X_i)} = \\frac{1}{\\sum_{i = 1}^N W^2(X^i)}\n",
    "$$\n",
    "\n",
    "To compare different approximations one may also be interested in $\\frac{\\text{ESS}}{N} \\cdot 100\\%$, the percentage of effective samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jaxtyping import Float, Array\n",
    "\n",
    "\n",
    "def ess(\n",
    "    normalized_weights: Float[Array, \"N\"]  # normalized weights\n",
    ") -> Float:  # the effective sample size\n",
    "    \"\"\"Compute the effective sample size of a set of normalized weights\"\"\"\n",
    "    return 1 / (normalized_weights**2).sum()\n",
    "\n",
    "\n",
    "def ess_lw(\n",
    "    log_weights: Float[Array, \"N\"]  # the log weights\n",
    ") -> Float:  # the effective sample size\n",
    "    \"\"\"Compute the effective sample size of a set of log weights\"\"\"\n",
    "    return ess(normalize_weights(log_weights))\n",
    "\n",
    "\n",
    "def ess_pct(\n",
    "    log_weights: Float[Array, \"N\"]  # log weights\n",
    ") -> Float:  # the effective sample size in percent, also called efficiency factor\n",
    "    (N,) = log_weights.shape\n",
    "    return ess_lw(log_weights) / N * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess(weights), ess_pct(lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "@jit\n",
    "def mc_integration(samples: Float[Array, \"N ...\"], log_weights: Float[Array, \"N\"]):\n",
    "    return jnp.einsum(\"i...,i->...\", samples, normalize_weights(log_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = mc_integration(samples, lw)\n",
    "true_signal = (model.B @ x[..., None]).squeeze(axis=-1)\n",
    "plt.plot(true_signal, label=\"true signal\")\n",
    "plt.plot(sample_mean, label=\"estimated mean signal\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "For prediction we are interested in the conditional expectations\n",
    "$$\n",
    "\\mathbf E \\left(Y_{n + t} | Y_0, \\dots, Y_n\\right),\n",
    "$$\n",
    "where we assume that the PGSSM has some known continuation after time $n$. \n",
    "We can estimate this conditional expectation by importance sampling. Given our samples $X^{i}_n$, $i = 1, \\dots, N$, we simulate forward in time to obtain $X^i_{n + t}$. We may then estimate the conditional expectation by \n",
    "$$\n",
    "\\sum_{i = 1}^N W^i \\mathbf E \\left( Y_{n + t} | X_{n + 1} = X^i_{n + t}\\right),\n",
    "$$\n",
    "by the dependency structure of the model ($Y_{n + t}$ is independent of $Y_0, \\dots, Y_n$ given $X_{t + n}$.)\n",
    "\n",
    "For prediction intervals, we follow the strategy provided by [@Durbin2012Time], Chapter 11.5.3: sort the univariate predictions, and the corresponding weights in the same order. Then the ECDF at $Y^i$, can be estimated as\n",
    "$$\n",
    "\\sum_{j = 1}^i W^i.\n",
    "$$\n",
    "Linearly interpolating between these values gives an ECDF which we use to create prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.typing import GLSSMProposal, GLSSMState\n",
    "from isssm.kalman import kalman\n",
    "from isssm.glssm import simulate_states\n",
    "from isssm.util import mm_time_sim\n",
    "from jax import jit\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def future_prediction_interval(dist, signal_samples, xi, log_weights, p):\n",
    "    def integer_ecdf(y):\n",
    "        return (\n",
    "            dist(signal_samples, xi).cdf(y).squeeze(axis=-1)\n",
    "            * normalize_weights(log_weights)\n",
    "        ).sum()\n",
    "\n",
    "    def ecdf(y):\n",
    "        y_floor = jnp.floor(y)\n",
    "        y_ceil = jnp.ceil(y)\n",
    "        y_gauss = y - y_floor\n",
    "\n",
    "        return integer_ecdf(y_floor) * (1 - y_gauss) + integer_ecdf(y_ceil) * y_gauss\n",
    "\n",
    "    def pinball_loss(y, p):\n",
    "        return (jnp.abs(ecdf(y) - p).sum()) ** 2\n",
    "\n",
    "    mean = mc_integration(dist(signal_samples, xi).mean(), log_weights)\n",
    "    result = minimize(pinball_loss, mean, args=(p,), method=\"Nelder-Mead\")\n",
    "    return result.x\n",
    "\n",
    "\n",
    "def _prediction_percentiles(Y, weights, probs):\n",
    "    Y_sorted = jnp.sort(Y)\n",
    "    weights_sorted = weights[jnp.argsort(Y)]\n",
    "    cumsum = jnp.cumsum(weights_sorted)\n",
    "\n",
    "    # find indices of cumulative sum closest to probs\n",
    "    # take corresponding Y_sorted values\n",
    "    # with linear interpolation if necessary\n",
    "\n",
    "    indices = jnp.searchsorted(cumsum, probs)\n",
    "    indices = jnp.clip(indices, 1, len(Y_sorted) - 1)\n",
    "    left_indices = indices - 1\n",
    "    right_indices = indices\n",
    "    left_cumsum = cumsum[left_indices]\n",
    "    right_cumsum = cumsum[right_indices]\n",
    "    left_Y = Y_sorted[left_indices]\n",
    "    right_Y = Y_sorted[right_indices]\n",
    "    # linear interpolation\n",
    "    quantiles = left_Y + (probs - left_cumsum) / (right_cumsum - left_cumsum) * (\n",
    "        right_Y - left_Y\n",
    "    )\n",
    "    return quantiles\n",
    "\n",
    "\n",
    "prediction_percentiles = vmap(\n",
    "    vmap(_prediction_percentiles, (1, None, None), 1), (2, None, None), 2\n",
    ")\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: PGSSM,\n",
    "    y: Float[Array, \"n+1 p\"],\n",
    "    proposal: GLSSMProposal,\n",
    "    future_model: PGSSM,\n",
    "    N: int,\n",
    "    key: PRNGKeyArray,\n",
    "):\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    signal_samples, log_weights = pgssm_importance_sampling(\n",
    "        y, model, proposal.z, proposal.Omega, N, subkey\n",
    "    )\n",
    "    (N,) = log_weights.shape\n",
    "\n",
    "    signal_model = GLSSM(\n",
    "        proposal.u,\n",
    "        proposal.A,\n",
    "        proposal.D,\n",
    "        proposal.Sigma0,\n",
    "        proposal.Sigma,\n",
    "        proposal.v,\n",
    "        proposal.B,\n",
    "        proposal.Omega,\n",
    "    )\n",
    "\n",
    "    @jit\n",
    "    def future_sample(signal_sample, key):\n",
    "        x_filt, Xi_filt, _, _ = kalman(signal_sample, signal_model)\n",
    "        state = GLSSMState(\n",
    "            future_model.u.at[0].set(x_filt[-1]),\n",
    "            future_model.A,\n",
    "            future_model.D,\n",
    "            Xi_filt[-1],\n",
    "            future_model.Sigma,\n",
    "        )\n",
    "\n",
    "        (x,) = simulate_states(state, 1, key)\n",
    "        return x\n",
    "\n",
    "    key, *subkeys = jrn.split(key, N + 1)\n",
    "    subkeys = jnp.array(subkeys)\n",
    "\n",
    "    future_x = vmap(future_sample)(signal_samples, subkeys)\n",
    "    future_s = mm_time_sim(future_model.B, future_x)\n",
    "    future_y = future_model.dist(future_s, future_model.xi).mean()\n",
    "\n",
    "    return (future_x, future_s, future_y), log_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrn.split(key)\n",
    "n_ahead = 10\n",
    "ten_steps_ahead_model = PGSSM(\n",
    "    model.u[: n_ahead + 1],\n",
    "    model.A[:n_ahead],\n",
    "    model.D[:n_ahead],\n",
    "    model.Sigma0,\n",
    "    model.Sigma[:n_ahead],\n",
    "    model.v[: n_ahead + 1],\n",
    "    model.B[: n_ahead + 1],\n",
    "    model.dist,\n",
    "    model.xi[: n_ahead + 1],\n",
    ")\n",
    "(_, s_pred, y_pred), log_weights_pred = predict(\n",
    "    model, y, proposal, ten_steps_ahead_model, 1000, subkey\n",
    ")\n",
    "\n",
    "mean_y = (y_pred * normalize_weights(log_weights_pred)[:, None, None]).sum()\n",
    "past_inds = jnp.arange(y.shape[0])\n",
    "future_inds = jnp.arange(y.shape[0], y.shape[0] + n_ahead)\n",
    "percentiles = prediction_percentiles(\n",
    "    y_pred, normalize_weights(log_weights_pred), jnp.array([0.1, 0.5, 0.9])\n",
    ")\n",
    "lower, mid, upper = percentiles\n",
    "plt.plot(past_inds, y, label=\"observed\")\n",
    "plt.plot(future_inds, mid[1:], label=\"median\")\n",
    "plt.plot(\n",
    "    future_inds,\n",
    "    lower[1:],\n",
    "    linestyle=\"--\",\n",
    "    color=\"grey\",\n",
    "    label=\"80% prediction interval\",\n",
    ")\n",
    "plt.plot(future_inds, upper[1:], linestyle=\"--\", color=\"grey\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.kalman import to_signal_model\n",
    "\n",
    "\n",
    "def prediction(\n",
    "    f: callable,\n",
    "    y,\n",
    "    proposal: GLSSMProposal,\n",
    "    model: PGSSM,\n",
    "    N: int,\n",
    "    key: PRNGKeyArray,\n",
    "    probs: Float[Array, \"k\"],\n",
    "    prediction_model=None,\n",
    "):\n",
    "    if prediction_model is None:\n",
    "        prediction_model = model\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    signal_samples, log_weights = pgssm_importance_sampling(\n",
    "        y, model, proposal.z, proposal.Omega, N, subkey\n",
    "    )\n",
    "    N = signal_samples.shape[0]\n",
    "\n",
    "    signal_model = to_signal_model(proposal)\n",
    "\n",
    "    def state_sample(signal_sample, key):\n",
    "        (x_sample,) = FFBS(signal_sample, signal_model, 1, key)\n",
    "        return x_sample\n",
    "\n",
    "    key, *subkeys = jrn.split(key, N + 1)\n",
    "    subkeys = jnp.array(subkeys)\n",
    "    x_samples = vmap(state_sample)(signal_samples, subkeys)\n",
    "    s_samples = mm_time_sim(prediction_model.B, x_samples)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    y_prime_samples = prediction_model.dist(\n",
    "        s_samples, prediction_model.xi[None]\n",
    "    ).sample(seed=subkey)\n",
    "\n",
    "    f_samples = vmap(f)(x_samples, signal_samples, y_prime_samples)\n",
    "\n",
    "    mean_f = mc_integration(f_samples, log_weights)\n",
    "    sd_f = jnp.sqrt(mc_integration(f_samples**2, log_weights) - mean_f**2)\n",
    "\n",
    "    if f_samples.ndim == 3:\n",
    "        percentiles = prediction_percentiles(\n",
    "            f_samples, normalize_weights(log_weights), probs\n",
    "        )\n",
    "    elif f_samples.ndim == 2:\n",
    "        percentiles = vmap(_prediction_percentiles, (1, None, None), 1)(\n",
    "            f_samples, normalize_weights(log_weights), probs\n",
    "        )\n",
    "    elif f_samples.ndim == 1:\n",
    "        percentiles = _prediction_percentiles(\n",
    "            f_samples, normalize_weights(log_weights), probs\n",
    "        )\n",
    "\n",
    "    return mean_f, sd_f, percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, s, y_prime):\n",
    "    return y_prime[:, 0:1]\n",
    "\n",
    "\n",
    "key, subkey = jrn.split(key)\n",
    "mean, sd, quants = prediction(\n",
    "    f, y, proposal, model, 10000, subkey, jnp.array([0.1, 0.5, 0.9])\n",
    ")\n",
    "\n",
    "plt.plot(y, label=\"observed\")\n",
    "plt.plot(mean, label=\"predicted\")\n",
    "plt.plot(quants[0], linestyle=\"--\", color=\"grey\", label=\"prediction interval\")\n",
    "plt.plot(quants[2], linestyle=\"--\", color=\"grey\")\n",
    "plt.xlim(90, 100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants[2, 99], y[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates.jax.distributions import NegativeBinomial\n",
    "\n",
    "NegativeBinomial(20, jnp.log(356) - jnp.log(20)).cdf(557)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_prediction_interval(\n",
    "    model.dist, s_pred[:, 0], model.xi[0], log_weights_pred, 0.5\n",
    "), mid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
