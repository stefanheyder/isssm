{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "skip_doc: true\n",
    "skip_test: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp estimation\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=section.3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "from jax import vmap\n",
    "from jaxtyping import Float, Array, PRNGKeyArray\n",
    "from isssm.kalman import kalman\n",
    "from jax import jit\n",
    "from scipy.optimize import minimize as minimize_scipy\n",
    "from isssm.laplace_approximation import laplace_approximation\n",
    "from isssm.modified_efficient_importance_sampling import (\n",
    "    modified_efficient_importance_sampling,\n",
    ")\n",
    "from isssm.importance_sampling import normalize_weights\n",
    "from isssm.typing import GLSSM, PGSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import jax\n",
    "from isssm.glssm import simulate_glssm\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "import fastcore.test as fct\n",
    "from isssm.pgssm import simulate_pgssm, nb_pgssm_running_example\n",
    "from isssm.models.glssm import lcm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian linear models\n",
    "\n",
    "For [GLSSMs](./00_glssm.ipynb) we can evaluate the likelihood analytically with a single pass of the Kalman filter.\n",
    "Based on the predictions $\\hat Y_{t| t - 1}$ and associated covariance matrices $\\Psi_{t + 1 | t}$ for $t = 0, \\dots n$ produced by the Kalman filter we can derive the gaussian negative log- likelihood which is given by the gaussian distribution with that mean and covariance matrix and observation $Y_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.util import MVN_degenerate as MVN\n",
    "\n",
    "vmm = vmap(jnp.matmul, (0, 0))\n",
    "\n",
    "\n",
    "@jit\n",
    "def gnll(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    x_pred: Float[Array, \"n+1 m\"],  # predicted states $\\hat X_{t+1\\bar t}$\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],  # predicted state covariances $\\Xi_{t+1\\bar t}$\n",
    "    B: Float[Array, \"n+1 p m\"],  # state observation matrices $B_{t}$\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # observation covariances $\\Omega_{t}$\n",
    ") -> Float:  # gaussian negative log-likelihood\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    y_pred = vmm(B, x_pred)\n",
    "    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega\n",
    "\n",
    "    return -MVN(y_pred, Psi_pred).log_prob(y).sum()\n",
    "\n",
    "\n",
    "@jit\n",
    "def gnll_full(y: Float[Array, \"n+1 p\"], model: GLSSM):  # observations $y_t$\n",
    "    filtered = kalman(y, model)\n",
    "    return gnll(y, filtered.x_pred, filtered.Xi_pred, model.B, model.Omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "glssm_model = lcm(1, 0.0, 1.0, 1.0, 1.0)\n",
    "_, (y,) = simulate_glssm(glssm_model, 1, jrn.PRNGKey(34234))\n",
    "\n",
    "x_filt, Xi_filt, x_pred, Xi_pred = kalman(y, glssm_model)\n",
    "nll = gnll_full(y, glssm_model)\n",
    "\n",
    "\n",
    "EY = jnp.zeros((2,))\n",
    "CovY = jnp.array([[2.0, 1.0], [1.0, 3.0]])\n",
    "\n",
    "fct.test_close(\n",
    "    nll, -tfd.MultivariateNormalFullCovariance(EY, CovY).log_prob(y.reshape(-1))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE in GLSSMs\n",
    "\n",
    "For a parametrized GLSSM, that is a model that depends on parameters $\\theta$, we can use numerical optimization to find the maximum likelihood estimatior.\n",
    "\n",
    "::: {.callout-caution}\n",
    "With these methods, the user has to take care that they provide a parametrization that is unconstrained, i.e. using $\\log$ transformations for positive parameters.\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "## Implementation Details\n",
    "\n",
    "1. For low dimensional state space models obtaining the gradient of the negative log likelihood may be feasible by automatic differentiation, in this case use the `mle_glssm_ad` method. Otherwise the derivative free Nelder-Mead method in `mle_glssm` may be favorable.\n",
    "2. To stabilize numerical results we minimize $\\frac{1}{(n + 1)p} \\log_{\\theta} p(y)$ instead of $\\log p_\\theta (y)$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from scipy.optimize import minimize as minimize_scipy\n",
    "from jax.scipy.optimize import minimize as minimize_jax\n",
    "from scipy.optimize import OptimizeResult\n",
    "from jax.scipy.optimize import OptimizeResults\n",
    "\n",
    "\n",
    "def mle_glssm(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    model_fn,  # parameterize GLSSM\n",
    "    theta0: Float[Array, \"k\"],  # initial parameter guess\n",
    "    aux,  # auxiliary data for the model\n",
    "    options=None,  # options for the optimizer\n",
    ") -> OptimizeResult:  # result of MLE optimization\n",
    "    \"\"\"Maximum likelihood estimation for GLSSM\"\"\"\n",
    "\n",
    "    @jit\n",
    "    def f(theta: Float[Array, \"k\"]) -> Float:\n",
    "        model = model_fn(theta, aux)\n",
    "        # improve numerical stability by dividing by number of observations\n",
    "        n_obs = y.size\n",
    "        return gnll_full(y, model) / n_obs\n",
    "\n",
    "    return minimize_scipy(f, theta0, method=\"BFGS\", options=options)\n",
    "\n",
    "\n",
    "def mle_glssm_ad(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    model_fn,  # parameterize GLSSM\n",
    "    theta0: Float[Array, \"k\"],  # initial parameter guess\n",
    "    aux,  # auxiliary data for the model\n",
    "    options=None,  # options for the optimizer\n",
    ") -> OptimizeResults:  # result of MLE optimization\n",
    "    \"\"\"Maximum likelihood estimation for GLSSM using automatic differentiation\"\"\"\n",
    "\n",
    "    def f(theta: Float[Array, \"k\"]) -> Float:\n",
    "        model = model_fn(theta, aux)\n",
    "        # improve numerical stability by dividing by number of observations\n",
    "        n_obs = y.size\n",
    "        return gnll_full(y, model) / n_obs\n",
    "\n",
    "    return minimize_jax(f, theta0, method=\"BFGS\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterized_lcm(theta, aux):\n",
    "    log_s2_eps, log_s2_eta = theta\n",
    "    n, x0, s2_x0 = aux\n",
    "\n",
    "    return lcm(n, x0, s2_x0, jnp.exp(log_s2_eps), jnp.exp(log_s2_eta))\n",
    "\n",
    "\n",
    "theta = jnp.log(jnp.array([2.0, 3.0]))\n",
    "aux = (100, 0.0, 1.0)\n",
    "true_model = parameterized_lcm(theta, aux)\n",
    "_, (y,) = simulate_glssm(true_model, 1, jrn.PRNGKey(15435324))\n",
    "\n",
    "# start far away from true parameter\n",
    "result_bfgs = mle_glssm(\n",
    "    y, parameterized_lcm, 2 * jnp.ones(2), aux, options={\"return_all\": True}\n",
    ")\n",
    "result_ad = mle_glssm_ad(y, parameterized_lcm, 2 * jnp.ones(2), aux)\n",
    "\n",
    "result_bfgs.x - result_ad.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "%timeit mle_glssm(y, parameterized_lcm, 2 * jnp.ones(2), aux)\n",
    "%timeit mle_glssm_ad(y, parameterized_lcm, 2 * jnp.ones(2), aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical differentiation is much faster here, and as accurate as automatic differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d grid on the log scale\n",
    "k = 21  # number of evaluations in each dimension\n",
    "log_s2_eps, log_s2_eta = jnp.meshgrid(\n",
    "    jnp.linspace(-3, 3, k) + theta[0], jnp.linspace(-3, 3, k) + theta[1]\n",
    ")\n",
    "# flatten\n",
    "thetas = jnp.vstack([log_s2_eps.ravel(), log_s2_eta.ravel()]).T\n",
    "\n",
    "\n",
    "def gnll_theta(theta):\n",
    "    return gnll_full(y, parameterized_lcm(theta, aux))\n",
    "\n",
    "\n",
    "nlls = vmap(gnll_theta)(thetas)\n",
    "# location of minium in nlls\n",
    "i = jnp.argmin(nlls)\n",
    "# location of minimum in the grid\n",
    "i_eps, i_eta = i // 21, i % 21\n",
    "\n",
    "plt.contourf(log_s2_eps, log_s2_eta, nlls.reshape(k, k), alpha=0.5)\n",
    "plt.scatter(\n",
    "    log_s2_eps[i_eps, i_eta],\n",
    "    log_s2_eta[i_eps, i_eta],\n",
    "    c=\"white\",\n",
    "    marker=\"x\",\n",
    "    label=\"min_grid\",\n",
    ")\n",
    "plt.scatter(theta[0], theta[1], c=\"r\", marker=\"x\", label=\"true\")\n",
    "plt.scatter(*result_bfgs.x, c=\"g\", marker=\"x\", label=\"$\\\\hat\\\\theta$\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\\\log(\\\\sigma^2_\\\\varepsilon)$\")\n",
    "plt.ylabel(\"$\\\\log(\\\\sigma^2_\\\\eta)$\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for Log-Concave State Space Models\n",
    "\n",
    "For non-gaussian state space models we cannot evaluate the likelihood analytically but resort to simulation methods, more specifically [importance sampling](40_importance_sampling.ipynb). \n",
    "\n",
    "Importance Sampling is performed using a surrogate gaussian model that shares the state density $g(x) = p(x)$ and is parameterized by synthetic observations $z$ and their covariance matrices $\\Omega$. In this surrogate model the likeilhood $\\ell_g = g(z)$ and posterior distribution $g(x|z)$ are tractable and we can [simulate from the posterior](10_kalman_filter_smoother#Sampling_from_the_smoothing_distribution).\n",
    "\n",
    "Having obtained $N$ independent samples $X^i, i= 1, \\dots, N$ from this surrogate posterior we can evaluate the likelihood $\\ell$ by Monte-Carlo integration:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(y) &= \\int p(x, y) \\,\\mathrm dx \\\\\n",
    "    &=\\int \\frac{p(x,y)}{g(x|z)} g(x|z) \\,\\mathrm dx \\\\\n",
    "    &= g(z) \\int \\frac{p(y|x)}{g(z|x)} g(x|z)\\,\\mathrm dx \\\\\n",
    "    &\\approx g(z) \\frac 1 N \\sum_{i =1}^N w(X^i) \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $w(X^i) = \\frac{p\\left(y|X^i\\right)}{g\\left(z|X^i\\right)}$ are the unnormalized importance sampling weights. \n",
    "Additionally, we use the bias correction term $ \\frac{s^2_w}{2 N \\bar w^2}$ from [@Durbin1997Monte], where $s^2_w$ is the empirical variance of the weights and $\\bar w$ is their mean.\n",
    "\n",
    "In total we estimate the negative log-likelihood by\n",
    "\n",
    "$$\n",
    "- \\log p(y) \\approx \\ell_g - \\log \\left(\\sum_{i=1}^N w(X^i) \\right) + \\log N - \\frac{s^{2}_{w}}{2 N \\bar w^{2}}\n",
    "$$\n",
    "\n",
    "::: {.callout-note}\n",
    "### Implementation Details\n",
    "\n",
    "1. Similar to MLE in GLSSMs, we minimize $-\\frac{1}{(n + 1)p} \\log p(y)$ instead of $-\\log p(y)$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from jax.scipy.special import logsumexp\n",
    "from isssm.importance_sampling import pgssm_importance_sampling\n",
    "from isssm.kalman import kalman\n",
    "from isssm.typing import GLSSM, PGSSM\n",
    "\n",
    "\n",
    "def _pgnll(\n",
    "    gnll: Float,  # surrogate gaussian negative log-likelihood\n",
    "    unnormalized_log_weights: Float[\n",
    "        Array, \"N\"\n",
    "    ],  # unnormalized log-weights $\\log w(X^i)$\n",
    ") -> Float:  # the approximate negative log-likelihood\n",
    "    \"\"\"Internal Log-Concave Negative Log-Likelihood\"\"\"\n",
    "    (N,) = unnormalized_log_weights.shape\n",
    "    weights = normalize_weights(unnormalized_log_weights)\n",
    "    return (\n",
    "        gnll - logsumexp(unnormalized_log_weights) + jnp.log(N)\n",
    "    )  # - (jnp.var(weights) / (2 * N * jnp.mean(weights) ** 2))\n",
    "\n",
    "\n",
    "def pgnll(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # the model\n",
    "    z: Float[Array, \"n+1 p\"],  # synthetic observations\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # covariance of synthetic observations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    ") -> Float:  # the approximate negative log-likelihood\n",
    "    \"\"\"Log-Concave Negative Log-Likelihood\"\"\"\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "\n",
    "    _, log_weights = pgssm_importance_sampling(y, model, z, Omega, N, subkey)\n",
    "\n",
    "    return _pgnll(\n",
    "        gnll_full(\n",
    "            z,\n",
    "            GLSSM(\n",
    "                model.u,\n",
    "                model.A,\n",
    "                model.D,\n",
    "                model.Sigma0,\n",
    "                model.Sigma,\n",
    "                model.v,\n",
    "                model.B,\n",
    "                Omega,\n",
    "            ),\n",
    "        ),\n",
    "        log_weights,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "fct.test_close(_pgnll(0.0, jnp.array([0.0, 0.0])), 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform maximum likelihood estimation in a parameterized log-concave state space model we have to evaluate the likelihood several times. For evaluating the likelihood at $\\theta$ we have to perform the following:\n",
    "\n",
    "1. Find a surrogate Gaussian model $g(x,z)$ for $p_\\theta(x,y)$ (e.g. [Laplace approximation](30_laplace_approximation.ipynb) or [efficient importance sampling](50_modified_efficient_importance_sampling.ipynb)).\n",
    "2. Generate [importance samples](40_importance_sampling.ipynb) from these models using the [Kalman smoother](10_kalman_filter_smoother.ipynb).\n",
    "3. Approximate the negative log likelihood using the methods of this module.\n",
    "\n",
    "This makes maximum likelihood an intensive task for these kinds of models.\n",
    "\n",
    "For an initial guess we optimize the approximatie loglikelihood with the weights component fixed at the mode, see Eq. (21) in[@Durbin1997Monte] for further details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.importance_sampling import log_weights\n",
    "from isssm.laplace_approximation import posterior_mode\n",
    "\n",
    "\n",
    "def initial_theta(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    model_fn,  # parameterized PGSSM\n",
    "    theta0: Float[Array, \"k\"],  # initial parameter guess\n",
    "    aux,  # auxiliary data for the model\n",
    "    n_iter_la: int,  # number of LA iterations\n",
    "    options=None,  # options for the optimizer\n",
    "    jit_target=True,  # whether to jit the function\n",
    "):\n",
    "    \"\"\"Initial value for Maximum Likelihood Estimation for PGSSMs\"\"\"\n",
    "\n",
    "    def f(theta):\n",
    "        model = model_fn(theta, aux)\n",
    "\n",
    "        proposal, info = laplace_approximation(y, model, n_iter_la)\n",
    "\n",
    "        u, A, D, Sigma0, Sigma, v, B, Omega, z = proposal\n",
    "        glssm_la = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "        signal = posterior_mode(proposal)\n",
    "        _, _, x_pred, Xi_pred = kalman(z, glssm_la)\n",
    "\n",
    "        negloglik = gnll(z, x_pred, Xi_pred, B, Omega) - log_weights(\n",
    "            signal, y, model.dist, model.xi, z, Omega\n",
    "        )\n",
    "        # improve numerical stability by dividing by number of observations\n",
    "        n_obs = y.size\n",
    "        return negloglik / n_obs\n",
    "\n",
    "    if jit_target:\n",
    "        f = jit(f)\n",
    "\n",
    "    result = minimize_scipy(f, theta0, method=\"BFGS\", jac=\"3-point\", options=options)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example consider a parameterized version of the [running example](20_lcssm.ipynb#running) with unknown parameters $\\sigma^2_\\varepsilon$ and $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(theta, aux) -> PGSSM:\n",
    "    log_s2_eps, log_r = theta\n",
    "\n",
    "    n, x0 = aux\n",
    "\n",
    "    r = jnp.exp(log_r)\n",
    "    s2_eps = jnp.exp(log_s2_eps)\n",
    "    return nb_pgssm_running_example(\n",
    "        s_order=0,\n",
    "        Sigma0_seasonal=jnp.eye(0),\n",
    "        x0_seasonal=jnp.zeros(0),\n",
    "        s2_speed=s2_eps,\n",
    "        r=r,\n",
    "        n=n,\n",
    "    )\n",
    "\n",
    "\n",
    "n = 100\n",
    "theta_lc = jnp.array([jnp.log(1), jnp.log(20.0)])\n",
    "aux = (n, jnp.ones(2))\n",
    "model = model_fn(theta_lc, aux)\n",
    "key = jrn.PRNGKey(512)\n",
    "key, subkey = jrn.split(key)\n",
    "_, (y,) = simulate_pgssm(model, 1, subkey)\n",
    "theta_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_result = initial_theta(y, model_fn, theta_lc, aux, 10)\n",
    "theta0 = initial_result.x\n",
    "initial_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "%timeit initial_theta(y, model_fn, theta_lc, aux, 10, jit_target=True)\n",
    "%timeit initial_theta(y, model_fn, theta_lc, aux, 10, jit_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def mle_pgssm(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    model_fn,  # parameterized LCSSM\n",
    "    theta0: Float[Array, \"k\"],  # initial parameter guess\n",
    "    aux,  # auxiliary data for the model\n",
    "    n_iter_la: int,  # number of LA iterations\n",
    "    N: int,  # number of importance samples\n",
    "    key: Array,  # random key\n",
    "    options=None,  # options for the optimizer\n",
    ") -> Float[Array, \"k\"]:  # MLE\n",
    "    \"\"\"Maximum Likelihood Estimation for PGSSMs\"\"\"\n",
    "\n",
    "    @jit\n",
    "    def f(theta, key):\n",
    "        model = model_fn(theta, aux)\n",
    "\n",
    "        proposal_la, _ = laplace_approximation(y, model, n_iter_la)\n",
    "\n",
    "        key, subkey = jrn.split(key)\n",
    "        proposal_meis, _ = modified_efficient_importance_sampling(\n",
    "            y, model, proposal_la.z, proposal_la.Omega, n_iter_la, N, subkey\n",
    "        )\n",
    "\n",
    "        key, subkey = jrn.split(key)\n",
    "        # improve numerical stability by dividing by number of observations\n",
    "        n_obs = y.size\n",
    "        return pgnll(y, model, proposal_meis.z, proposal_meis.Omega, N, subkey) / n_obs\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    result = minimize_scipy(\n",
    "        f, theta0, method=\"BFGS\", jac=\"3-point\", options=options, args=(subkey,)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrn.split(key)\n",
    "result = mle_pgssm(y, model_fn, theta0, aux, 10, 1000, subkey)\n",
    "theta_hat = result.x\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pgnll_full(theta, key):\n",
    "    model = model_fn(theta, aux)\n",
    "\n",
    "    proposal, info = laplace_approximation(y, model, 10)\n",
    "    key, subkey = jrn.split(key)\n",
    "    proposal_meis, _ = modified_efficient_importance_sampling(\n",
    "        y, model, proposal.z, proposal.Omega, 10, 100, subkey\n",
    "    )\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    return pgnll(y, model, proposal_meis.z, proposal_meis.Omega, 100, subkey) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d grid on the log scale\n",
    "(log_sigma_min, log_r_min), (log_sigma_max, log_r_max) = jnp.min(\n",
    "    jnp.vstack((theta_lc, theta0, theta_hat)), axis=0\n",
    "), jnp.max(jnp.vstack((theta_lc, theta0, theta_hat)), axis=0)\n",
    "k = 20  # number of evaluations in each dimension\n",
    "delta = 0.5\n",
    "log_sigma, log_r = jnp.meshgrid(\n",
    "    jnp.linspace(log_sigma_min - delta, log_sigma_max + delta, k),\n",
    "    jnp.linspace(log_r_min - delta, log_r_max + delta, k),\n",
    ")\n",
    "# flatten\n",
    "thetas = jnp.vstack([log_sigma.ravel(), log_r.ravel()]).T\n",
    "\n",
    "key, subkey = jrn.split(key)\n",
    "nlls = vmap(pgnll_full, (0, None))(thetas, subkey)\n",
    "# location of minimum in nlls\n",
    "i = jnp.argmin(nlls)\n",
    "# location of minimum in the grid\n",
    "i_sigma, i_r = i // k, i % k\n",
    "\n",
    "plt.contourf(log_sigma, log_r, nlls.reshape(k, k))\n",
    "plt.scatter(\n",
    "    log_sigma[i_sigma, i_r],\n",
    "    log_r[i_sigma, i_r],\n",
    "    c=\"white\",\n",
    "    marker=\"x\",\n",
    "    label=\"min_grid\",\n",
    ")\n",
    "plt.scatter(theta_lc[0], theta_lc[1], c=\"r\", marker=\"x\", label=\"true\")\n",
    "plt.scatter(theta0[0], theta0[1], c=\"black\", marker=\"x\", label=\"$\\\\theta_0$\")\n",
    "plt.scatter(*theta_hat, c=\"g\", marker=\"o\", label=\"min_mle\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$\\\\log(\\\\sigma^2_\\\\varepsilon)$\")\n",
    "plt.ylabel(\"$\\\\log(r)$\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above picture, we see that $\\log r$ is hard to determine: the likelihood is very flat in the $r$ direction, which explains the precision loss warning in the optimizer. Nevertheless, our estimate $\\hat\\theta$ seems to have converged to a reasonable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
