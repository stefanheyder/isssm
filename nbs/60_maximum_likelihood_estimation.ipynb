{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "from jax import vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from isssm.kalman import kalman\n",
    "from isssm.glssm_models import lcm\n",
    "from isssm.glssm import simulate_glssm\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "import fastcore.test as fct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian linear models\n",
    "\n",
    "For Gaussian linear state space models we can evaluate the likelihood analytically with a single pass of the Kalman filter.\n",
    "Based on the predictions $\\hat Y_{t| t - 1}$ and associated covariance matrices $\\Psi_{t + 1 | t}$ for $t = 0, \\dots n$ produced by the Kalman filter we can derive the gaussian negative log likelihood which is given by the gaussian distribution with that mean and covariance matrix and observation $Y_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jaxtyping import Float, Array\n",
    "\n",
    "vmm = vmap(jnp.matmul, (0, 0))\n",
    "\n",
    "\n",
    "def gnll(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    x_pred: Float[Array, \"n+1 m\"],  # predicted states $\\hat X_{t+1\\bar t}$\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],  # predicted state covariances $\\Xi_{t+1\\bar t}$\n",
    "    B: Float[Array, \"n+1 p m\"],  # state observation matrices $B_{t}$\n",
    "    Omega: Float[Array, \"n+1 p p\"],  # observation covariances $\\Omega_{t}$\n",
    ") -> Float:  # gaussian negative log-likelihood\n",
    "    \"\"\"Gaussian negative log-likelihood\"\"\"\n",
    "    y_pred = vmm(B, x_pred)\n",
    "    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega\n",
    "\n",
    "    return -tfd.MultivariateNormalFullCovariance(y_pred, Psi_pred).log_prob(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "x0, A, B, Sigma, Omega = lcm(1, 0., 1., 1., 1.)\n",
    "_, (y,) = simulate_glssm(x0, A, B, Sigma, Omega, 1, jrn.PRNGKey(34234))\n",
    "\n",
    "x_filt, Xi_filt, x_pred, Xi_pred = kalman(y, x0, Sigma, Omega, A, B)\n",
    "nll = gnll(y, x_pred, Xi_pred, B, Omega)\n",
    "\n",
    "\n",
    "EY = jnp.zeros((2,))\n",
    "CovY = jnp.array([[2., 1.], [1., 3.]])\n",
    "\n",
    "fct.test_eq(nll, -tfd.MultivariateNormalFullCovariance(EY, CovY).log_prob(y.reshape(-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE in GLSSMs\n",
    "\n",
    "For a parametrized GLSSM, that is a model that depends on parameters $\\theta$, we can use numerical optimization to find the maximum likelihood estimatior.\n",
    "\n",
    "::: {.callout-caution}\n",
    "With this method, the user has to take care that they provide a parametrization that is unconstrained, i.e. using $\\log$ transformations for positive parameters.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from jax.scipy.optimize import minimize, OptimizeResults\n",
    "\n",
    "def mle_glssm(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations $y_t$\n",
    "    model,  # parameterize GLSSM\n",
    "    theta0: Float[Array, \"k\"],  # initial parameter guess\n",
    "    aux,  # auxiliary data for the model\n",
    ") -> OptimizeResults:  # result of MLE optimization\n",
    "    \"\"\"Maximum likelihood estimation for GLSSM\"\"\"\n",
    "    def f(theta: Float[Array, \"k\"]) -> Float:\n",
    "        x0, A, B, Sigma, Omega = model(theta, aux)\n",
    "        _, _, x_pred, Xi_pred = kalman(y, x0, Sigma, Omega, A, B)\n",
    "        return gnll(y, x_pred, Xi_pred, B, Omega)\n",
    "\n",
    "    return minimize(f, theta0, method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([1.3734169, 3.719784 ], dtype=float32), Array([2., 3.], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parameterized_lcm(theta, aux):\n",
    "    log_s2_eps, log_s2_eta = theta\n",
    "    n, x0, s2_x0 = aux\n",
    "\n",
    "    return lcm(n, x0, s2_x0, jnp.exp(log_s2_eps), jnp.exp(log_s2_eta))\n",
    "    \n",
    "theta = jnp.log(jnp.array([2., 3.]))\n",
    "aux = (100, 0., 1.)\n",
    "x0, A, B, Sigma, Omega = parameterized_lcm(theta, aux)\n",
    "_, (y,) = simulate_glssm(x0, A, B, Sigma, Omega, 1, jrn.PRNGKey(15435324))\n",
    "\n",
    "result = mle_glssm(y, parameterized_lcm, jnp.ones(2), aux)\n",
    "jnp.exp(result.x), jnp.exp(theta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic behavior of MLE\n",
    "\n",
    "::: {.callout-note}\n",
    "# TODO\n",
    "Write / Implement\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
