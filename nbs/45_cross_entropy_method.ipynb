{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp ce_method\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax.random as jrn\n",
    "from jax import vmap, jit\n",
    "from isssm.importance_sampling import ess_pct\n",
    "from isssm.pgssm import log_prob as log_prob_joint\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from jaxtyping import Float, Array, PRNGKeyArray\n",
    "from typing import Tuple\n",
    "from isssm.importance_sampling import normalize_weights\n",
    "from isssm.util import converged\n",
    "from jax.lax import while_loop, fori_loop, scan\n",
    "from isssm.laplace_approximation import laplace_approximation\n",
    "from tensorflow_probability.substrates.jax.distributions import (\n",
    "    MultivariateNormalFullCovariance as MVN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import jax\n",
    "import fastcore.test as fct\n",
    "from isssm.pgssm import simulate_pgssm, nb_pgssm_running_example\n",
    "from isssm.importance_sampling import pgssm_importance_sampling\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy method \n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=subsection.3.6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!CAUTION]\n",
    "> this module is still under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy method [@Rubinstein1997Optimization;@Rubinstein2004CrossEntropy] is a method for determining good importance sampling proposals. Given a parametric family of proposals $g_\\theta(x)$ and target $p(x)$, the Cross-Entropy method aims at choosing $\\theta$ such that the Cross Entropy \n",
    "$$\n",
    "\\mathcal H_{\\text{CE}} \\left( p \\middle|\\middle| g_{\\theta} \\right) = \\int p(x) \\log g_{\\theta}(x) \\mathrm d x\n",
    "$$\n",
    "is maximized. This is equivalent to minimizing the Kullback Leibler divergence between $p$ and $g_\\theta$. As $H_\\text{CE}$ is not analytically available, it is approximated by importance sampling itself, usually with a suitable proposal $g_{\\hat\\theta_0}$. Then the approximate optimization problem is solved, yielding $\\hat \\theta_1$. These steps are then iterated until convergence, using common random numbers to ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the Cross-Entropy method with a Gaussian proposal $g_\\theta$, we see that the optimal $\\theta$ only depends on the first and second order moments of $p$, indeed the optimal Gaussian is the one that matches these moments. Unfortunately this approach is not feasible for the models we consider in this package as the dimensionality ($n \\cdot m$) is likely too high to act on the joint distribution directly - matching means is feasible, but simulating from the distribution and evaluating the likelihood is infeasible. However, we can exploit the Markov structure of our models:\n",
    "\n",
    "For the class of state space models treated in this package, it can be shown that the smoothing distribution, the target of our inference, $p(x|y)$, is again a Markov process, see Chapter 5 in [@Chopin2020Introduction], so it makes sense to approximate this distribution with a Gaussian Markov process. Thus, we only need to find the closest (in terms of KL-divergence) Gaussian Markov process, which is feasible, and can be obtained by choosing the approximation to match the mean and consecutive covariances, i.e. match \n",
    "$$\n",
    "    \\operatorname{Cov} \\left( (X_{t}, X_{t + 1}) \\right) \\in \\mathbf R^{2m \\times 2m}\n",
    "$$\n",
    "for all $t = 0, \\dots, n - 1$.  These are just $\\mathcal O(nm^2)$ many parameters, instead of the $\\mathcal O(n^2m^2)$ many parameters required to match the whole covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "from typing import NamedTuple\n",
    "\n",
    "import jax.scipy.linalg as jsla\n",
    "\n",
    "from isssm.typing import PGSSM, MarkovProposal, Observations\n",
    "from isssm.util import (\n",
    "    degenerate_cholesky,\n",
    "    location_antithetic,\n",
    "    mm_sim,\n",
    "    mm_time_sim,\n",
    "    scale_antithethic,\n",
    ")\n",
    "\n",
    "\n",
    "def proposal_from_moments(\n",
    "    mean: Float[Array, \"n+1 m\"],  # mean $v$\n",
    "    consecutive_covs: Float[\n",
    "        Array, \"n 2*m 2*m\"\n",
    "    ],  # covariances $\\operatorname{Cov}((U_t, U_{t + 1}))\n",
    ") -> MarkovProposal:  # corresponding proposal\n",
    "    \"\"\"Find the unique Gaussian Markov proposal that matches means and consecutive covariances.\"\"\"\n",
    "    _, m = mean.shape\n",
    "\n",
    "    # prepend Cov((0, U_0))\n",
    "    initial_cov = jsla.block_diag(jnp.zeros((m, m)), consecutive_covs[0][:m, :m])\n",
    "    all_covs = jnp.concatenate(\n",
    "        (\n",
    "            initial_cov[None],\n",
    "            consecutive_covs,\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    chols = degenerate_cholesky(all_covs)\n",
    "\n",
    "    # J_tt and J_tp1t dont matter for the first entry\n",
    "    J_tt = chols[1:, :m, :m]\n",
    "    J_tp1t = chols[1:, m:, :m]\n",
    "    Rs = chols[:, m:, m:]\n",
    "\n",
    "    return MarkovProposal(mean=mean, R=Rs, J_tt=J_tt, J_tp1t=J_tp1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that this produces the correct proposal, let us check it for a stationary AR(1) process with mean $0$ and recurrence\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X_{t + 1} = \\alpha X_{t} + \\varepsilon_{t} && \\varepsilon_{t} \\sim \\mathcal N(0, \\sigma^{2})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The initial variance is $R_{0}^2= \\tau^{2} = \\operatorname{Var} (X_0) = \\frac{\\sigma^{2}}{1 - \\alpha^{2}}$, the joint covarinces are \n",
    "$$\n",
    "    \\operatorname{Cov}(X_{t}, X_{t + 1}) = \n",
    "    \\tau^{2}\\begin{pmatrix}\n",
    "        1 & \\alpha \\\\\n",
    "        \\alpha & 1\n",
    "    \\end{pmatrix},\n",
    "$$\n",
    "with innovations covariance $R_{t}^2 = \\sigma^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 10, 1\n",
    "alpha = 0.5\n",
    "s2 = 1.0\n",
    "tau2 = s2 / (1 - alpha**2)\n",
    "mu = jnp.zeros((n + 1, 1))\n",
    "consecutive_covs = tau2 * jnp.broadcast_to(\n",
    "    jnp.array([[1.0, alpha], [alpha, 1.0]]), (n, 2, 2)\n",
    ")\n",
    "\n",
    "proposal = proposal_from_moments(mu, consecutive_covs)\n",
    "\n",
    "fct.test_eq(proposal.R.shape, (n + 1, 1, 1))\n",
    "fct.test_eq(proposal.J_tt.shape, (n, 1, 1))\n",
    "fct.test_eq(proposal.J_tp1t.shape, (n, 1, 1))\n",
    "\n",
    "fct.test_close(proposal.mean, mu)\n",
    "fct.test_close(proposal.R[0], jnp.sqrt(tau2) * jnp.eye(1))\n",
    "fct.test_close(proposal.R[1:], jnp.sqrt(s2) * jnp.eye(1))\n",
    "fct.test_close(proposal.J_tp1t / proposal.J_tt, jnp.full((n, 1, 1), alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "Given a Markov proposal, we can simulate from it by repeatedly applying its defining recurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def simulate_cem(\n",
    "    proposal: MarkovProposal,  # proposal\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random number seed\n",
    ") -> Float[Array, \"N n+1 m\"]:\n",
    "    np1, m = proposal.mean.shape\n",
    "    key, subkey = jrn.split(key)\n",
    "    u = MVN(loc=jnp.zeros(m), covariance_matrix=jnp.eye(m)).sample((N, np1), subkey)\n",
    "\n",
    "    # transpose to have time in first dimension\n",
    "    eps = mm_time_sim(proposal.R, u).transpose((1, 0, 2))\n",
    "    lower_tri_solve = partial(jsla.solve_triangular, lower=True)\n",
    "\n",
    "    def _iteration(carry, inputs):\n",
    "        (x_prev,) = carry\n",
    "        eps, J_tt, J_tp1t = inputs\n",
    "\n",
    "        x_next = mm_sim(J_tp1t, vmap(lower_tri_solve, (None, 0))(J_tt, x_prev)) + eps\n",
    "\n",
    "        return (x_next,), x_next\n",
    "\n",
    "    J_tt_ext = jnp.concatenate([jnp.eye(m)[None], proposal.J_tt], axis=0)\n",
    "    J_tp1t_ext = jnp.concatenate([jnp.eye(m)[None], proposal.J_tp1t], axis=0)\n",
    "    _, x = scan(_iteration, (jnp.zeros((N, m)),), (eps, J_tt_ext, J_tp1t_ext))\n",
    "\n",
    "    mean = proposal.mean\n",
    "    samples = x.transpose((1, 0, 2)) + proposal.mean\n",
    "\n",
    "    u = u.reshape((N, -1))\n",
    "    l_samples = location_antithetic(samples, mean)\n",
    "    s_samples = scale_antithethic(u, samples, mean)\n",
    "    ls_samples = scale_antithethic(u, l_samples, mean)\n",
    "\n",
    "    return jnp.concatenate((samples, l_samples, s_samples, ls_samples), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## probability density function\n",
    "For importance sampling we need to evaluate the pdf of the proposal. We do this by first substracting the mean $v$, $U = X - v$ and going back to the innovations\n",
    "$$\n",
    "    \\varepsilon_{t} = U_{t} - C_{t - 1}U_{t - 1} \\sim \\mathcal N(0, R_{t}R_{t}^T)\n",
    "$$\n",
    "where $\\varepsilon_0 = U_0$. These are jointly independent and so their pdf is easy to compute. j\n",
    "\n",
    "For the log-weights, note that we cannot use the same weights as for [MEIS](./50_modified_efficient_importance_sampling.ipynb) as $p(x) \\neq g(x)$. Instead we have to calculate \n",
    "$$\n",
    "    \\log w(x) = \\log p(x,y) - \\log g(x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.util import mm_time\n",
    "\n",
    "\n",
    "def log_pdf(x: Float[Array, \"n+1 m\"], proposal: MarkovProposal):\n",
    "    np1, m = x.shape\n",
    "\n",
    "    lower_tri_solve = partial(jsla.solve_triangular, lower=True)\n",
    "    centered = x - proposal.mean\n",
    "    eps = centered[1:] - mm_time(\n",
    "        proposal.J_tp1t, vmap(lower_tri_solve)(proposal.J_tt, centered[:-1])\n",
    "    )\n",
    "    eps = jnp.concatenate((centered[:1], eps), axis=0)\n",
    "\n",
    "    nu = vmap(lower_tri_solve)(proposal.R, eps)\n",
    "\n",
    "    return (\n",
    "        MVN(jnp.zeros(m), jnp.eye(m)).log_prob(nu).sum()\n",
    "        - jnp.log(vmap(jnp.diag)(proposal.R)).sum()\n",
    "    )\n",
    "\n",
    "\n",
    "def log_weight_cem(\n",
    "    x: Float[Array, \"n+1 m\"],  # point at which to evaluate the weights\n",
    "    y: Observations,  # observations\n",
    "    model: PGSSM,  # modle\n",
    "    proposal: MarkovProposal,  # proposal\n",
    ") -> Float:  # log weights\n",
    "    log_p = log_prob_joint(x, y, model)\n",
    "    log_g = log_pdf(x, proposal)\n",
    "\n",
    "    return log_p - log_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "prop_2 = proposal_from_moments(mu[:2], consecutive_covs[:1])\n",
    "\n",
    "xs = jrn.normal(jrn.PRNGKey(534), (2, 1))\n",
    "\n",
    "fct.test_eq(\n",
    "    log_pdf(xs, prop_2),\n",
    "    MVN(jnp.zeros(2), consecutive_covs[0]).log_prob(xs[:, 0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSM to Markov Model\n",
    "To initialize the Cross-Entropy method, we will use the Laplace approximation, see [30_laplace_approximation.ipynb]. This approximates the true posterior by the posterior of a Gaussian state space model. To initiate the Cross-entropy procedure, we determine the Cholesky root of this Gaussian posterior and use it as an initial value. To determine the diagonal and off-diagonal components of the Cholesky root, we calculate the joint covariance matrix $\\text{Cov} \\left( X_t, X_{t + 1} | Y_1, \\dots, Y_n \\right)$ using the Kalman smoother and the FFBS, which results in \n",
    "$$\n",
    "\\text{Cov} \\left( X_t, X_{t + 1} | Y_1, \\dots, Y_n \\right) = \\begin{pmatrix} \n",
    "\\Xi_{t|n} & \\Xi_{t|t} A_t^T \\Xi_{t + 1|t}^{-1} \\Xi_{t + 1|n} \\\\\n",
    "\\left(\\Xi_{t|t} A_t^T \\Xi_{t + 1|t}^{-1} \\Xi_{t + 1|n} \\right)^T & \\Xi_{t + 1 | n}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.typing import GLSSM\n",
    "from isssm.kalman import kalman, smoother\n",
    "\n",
    "\n",
    "def _joint_cov(Xi_smooth_t, Xi_smooth_tp1, Xi_filt_t, Xi_pred_tp1, A_t):\n",
    "    \"\"\"Joint covariance of conditional Markov process\"\"\"\n",
    "    off_diag = (\n",
    "        Xi_filt_t @ A_t.T @ jnp.linalg.pinv(Xi_pred_tp1, hermitian=True) @ Xi_smooth_tp1\n",
    "    )  # jnp.linalg.solve(Xi_pred_tp1, Xi_smooth_tp1)\n",
    "    return jnp.block([[Xi_smooth_t, off_diag], [off_diag.T, Xi_smooth_tp1]])\n",
    "\n",
    "\n",
    "def posterior_markov_proposal(\n",
    "    y: Observations, model: GLSSM  # observations  # model\n",
    ") -> MarkovProposal:  # Markov proposal of posterior X|Y\n",
    "    \"\"\"calculate the Markov proposal of the smoothing distribution using the Kalman smoother\"\"\"\n",
    "    filtered = kalman(y, model)\n",
    "    _, Xi_filter, _, Xi_pred = filtered\n",
    "    x_smooth, Xi_smooth = smoother(filtered, model.A)\n",
    "\n",
    "    covs = vmap(_joint_cov)(\n",
    "        Xi_smooth[:-1], Xi_smooth[1:], Xi_filter[:-1], Xi_pred[1:], model.A\n",
    "    )\n",
    "\n",
    "    return proposal_from_moments(x_smooth, covs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-Entropy Method\n",
    "\n",
    "Finally, we have all the ingredients together to apply the CE-method to perform importance sampling in a PGSSM with observations $y$. We start by calculating the LA, convert its posterior distribution to a Markov-proposal and then repeatedly sample and update the proposal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "from isssm.typing import GLSSM\n",
    "\n",
    "\n",
    "def cross_entropy_method(\n",
    "    model: PGSSM,  # model\n",
    "    y: Observations,  # observations\n",
    "    N: int,  # number of samples to use in the CEM\n",
    "    key: PRNGKeyArray,  # random number seed\n",
    "    n_iter: int,  # number of iterations\n",
    ") -> MarkovProposal:  # the CEM proposal\n",
    "    \"\"\"iteratively perform the CEM to find an optimal proposal\"\"\"\n",
    "    key, subkey_crn = jrn.split(key)\n",
    "\n",
    "    proposal, info = laplace_approximation(y, model, n_iter)\n",
    "    glssm_la = GLSSM(\n",
    "        model.u,\n",
    "        model.A,\n",
    "        model.D,\n",
    "        model.Sigma0,\n",
    "        model.Sigma,\n",
    "        model.v,\n",
    "        model.B,\n",
    "        proposal.Omega,\n",
    "    )\n",
    "    initial = posterior_markov_proposal(proposal.z, glssm_la)\n",
    "\n",
    "    def _iteration(i, vals):\n",
    "        proposal, _ = vals\n",
    "\n",
    "        model_log_weights = partial(log_weight_cem, y=y, model=model, proposal=proposal)\n",
    "\n",
    "        samples = simulate_cem(proposal, N, subkey_crn)\n",
    "\n",
    "        _N, np1, m = samples.shape\n",
    "\n",
    "        log_w = vmap(model_log_weights)(samples)\n",
    "        w = normalize_weights(log_w)\n",
    "\n",
    "        mean = jnp.sum(w[:, None, None] * samples, axis=0)\n",
    "        x_consecutive = jnp.concatenate((samples[:, :-1], samples[:, 1:]), axis=-1)\n",
    "\n",
    "        cov_IS = partial(jnp.cov, aweights=w, rowvar=False)\n",
    "\n",
    "        # ensure matrices (in 1D case jnp.cov reduces to (np1, m))\n",
    "        cov_shape = (np1 - 1, 2 * m, 2 * m)\n",
    "        consecutive_covs = vmap(cov_IS, 1)(x_consecutive).reshape(cov_shape)\n",
    "\n",
    "        new_proposal = proposal_from_moments(mean, consecutive_covs)\n",
    "\n",
    "        return new_proposal, log_w\n",
    "\n",
    "    final_proposal, log_w = fori_loop(\n",
    "        0, n_iter, _iteration, (initial, jnp.empty(4 * N))\n",
    "    )\n",
    "\n",
    "    return final_proposal, log_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform the CE-method on our example model. We'll set the number of observations somewhat lower than for EIS, as the CE-method is less performant in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import pgssm_importance_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_order = 5\n",
    "model = nb_pgssm_running_example(\n",
    "    n=100,\n",
    "    s_order=s_order,\n",
    "    Sigma0_seasonal=jnp.eye(s_order - 1),\n",
    "    x0_seasonal=jnp.zeros(s_order - 1),\n",
    ")\n",
    "key = jrn.PRNGKey(511)\n",
    "key, subkey = jrn.split(key)\n",
    "_, (y,) = simulate_pgssm(model, 1, subkey)\n",
    "proposal_la, info_la = laplace_approximation(y, model, 10)\n",
    "key, subkey = jrn.split(key)\n",
    "samples_la, log_w_la = pgssm_importance_sampling(\n",
    "    y, model, proposal_la.z, proposal_la.Omega, 1000, subkey\n",
    ")\n",
    "key, subkey = jrn.split(subkey)\n",
    "proposal, log_w = cross_entropy_method(model, y, 10000, subkey, 10)\n",
    "ess_pct(log_w), ess_pct(log_w_la)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CEM can improve on the LA, but requires more samples than MEIS to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
