{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp modified_efficient_importance_sampling\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Efficient Importance Sampling\n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=subsection.3.6.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Modified efficient importance sampling is used to improve on the [Laplace approximation](./30_laplace_approximation.ipynb). The goal is to minimize the variance of log-weights in importance sampling [@Richard2007Efficient,@Koopman2019Modified]. If the importance sampling model is a Gaussian one where observations are conditionally independent across time and space, its iterations reduce to a least squares problem which can be solved efficiently.\n",
    "\n",
    "$$\n",
    "\\int \\left(\\log p (y | x) - \\log g(y|x) - \\lambda \\right)^2 \\log p(x|y) \\mathrm d x\n",
    "$$\n",
    "where $\\lambda = \\mathbf E \\left(\\log p(y|x) - \\log g(z|x)| Y = y\\right)$. This is approximated by an importance sampling version \n",
    "\n",
    "$$\n",
    "\\sum_{i = 1}^N \\left(\\log p(y|X^i) - \\log g(z|X^i) - \\lambda\\right) w(X^i).\n",
    "$$\n",
    "\n",
    "Using gaussian proposals $g$ we have for signals $S_t = B_t X_t$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log g(z_{t}|s_{t}) &= -\\frac{1}{2} (z_{t} - s_{t})^T\\Omega^{-1}_{t}(z_{t} - s_{t}) - \\frac{p}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\Omega_{t}.\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\Omega_t = \\operatorname{diag} \\left( \\omega_{t,1}, \\dots, \\omega_{t,p}\\right)$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Due to the large dimension of the problem we solve it for each $t$ separately\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i = 1}^N(\\log p(y_t|s_t^{i}) - \\log g(z_{t}| s_t^{i}) - \\lambda_{t})^2 w(s_t^i) &= \\sum_{i = 1}^N\\left(\\log p(y_t|s_t^{i}) +\\frac{1}{2} (z_{t} - s_{t}^{i})^T\\Omega^{-1}_{t}(z_{t} - s_{t}^{i}) + \\frac{p}{2} \\log (2\\pi) + \\frac{1}{2} \\log \\det \\Omega_{t} - \\lambda_{t}\\right)^2 w(s_t^i) \n",
    "%&=  \\sum_{i = 1}^{N}(\\log p(y_t|s_t^{i}) - (- 2 \\Omega_t ^{-1}z_t)^{T} s^{i}_t -  (s^{i}_t)^T\\Omega_t^{-1}s^{i}_t - \\lambda_t - C_t)^2w(s_t^i) \\\\\n",
    "%&= \\sum_{i = 1}^N \\left(\\log p(y_t|s_t^{i}) - (s^{i}_t)^{T}(- 2 \\Omega_t ^{-1}z_t) + \\frac{1}{2} \\sum_{j = 1}^{p} (s^{i}_{t,j})^{2} \\frac{1}{\\omega_{t,j}} - \\lambda_t - C_t \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and minimized over the unknown parameters $\\left(z_t, \\Omega_t, \\lambda_t - C_t\\right)$, which is a weighted least squares setting with \"observations\" $\\log p(y_t|s_t)$.\n",
    "\n",
    "To perform the estimation memory efficient, we combine the FFBS algorithm (see [00_glssm.ipynb]) with the optimization procedure, so the memory requirement of this algorithm is $\\mathcal O(N)$ instead of $\\mathcal O(N\\cdot n)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "At time $t$ the problem can be formulated as the following constrained linear least squares problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\min_{x} (y - Ax)^{T} W (y - Ax) &= x^{T}A^{T}W Ax - 2 y^{T} W Ax + y^{T}Wy= \\frac{1}{2} x^{T}Qx + c^Tx + y^{T}Wy \\\\\n",
    "    \\text{subject to ~ ~ ~ ~ ~} & l \\leq x \\leq u\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y &=  \\left(\\log p(y_{t}| s_{y}^{1}), \\dots, \\log p(y_{t}|s_{t}^N)\\right)^{T} &\\in \\mathbf{R}^{N}\\\\\n",
    "    A &= \\left( \\mathbf 1_{(N, 1)}, S, -\\frac{1}{2} S^{2}  \\right) &\\in \\mathbf{R}^{N \\times (2p + 1)}\\\\\n",
    "    S = (S_{i,j}) &= s_{t,j}^i &\\in \\mathbf{R}^{N \\times p}\\\\\n",
    "    x &= \\left(C_{t}, \\frac{z_{t}^{1}}{\\omega_{t}^1}, \\dots, \\frac{z_{t}^p}{\\omega_{t}^p}, \\frac{1}{\\omega_{t}^1}, \\dots, \\frac{1}{\\omega_{t}^p}\\right) &\\in \\mathbf{R}^{2p + 1}\\\\\n",
    "    C_{t} &= \\lambda_{t} - \\frac{1}{2}\\log\\det \\Omega_{t} - \\frac{p}{2}\\log(2\\pi) - \\frac{1}{2} z_{t}^T\\Omega_{t}^{-1}z_{t} &\\in \\mathbf{R}\\\\\n",
    "    W &= \\operatorname{diag} \\left( w(s_{t}^1), \\dots, w(s_{t}^N)\\right) &\\in \\mathbf{R}^{N\\times N}\\\\\n",
    "    Q &= 2 A^{T}W A  &\\in \\mathbf{R}^{(2p + 1) \\times (2p + 1)}\\\\\n",
    "    c &= - 2 A^{T}Wy &\\in \\mathbf{R}^{2p+1}\n",
    "\\end{align*}\n",
    "$$\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "from jaxtyping import Float, Array, PRNGKeyArray\n",
    "from jax import vmap, jit\n",
    "from isssm.util import converged\n",
    "from isssm.importance_sampling import log_weights_t, normalize_weights\n",
    "from functools import partial\n",
    "from jax.lax import while_loop\n",
    "from isssm.kalman import kalman, simulation_smoother\n",
    "from jax.lax import scan\n",
    "from isssm.util import MVN_degenerate as MVN, mm_sim\n",
    "\n",
    "from isssm.glssm import mm_sim\n",
    "from isssm.typing import GLSSM, PGSSM, GLSSMProposal, ConvergenceInformation\n",
    "\n",
    "\n",
    "@jit\n",
    "def optimal_parameters(\n",
    "    signal: Float[Array, \"N p\"], weights: Float[Array, \"N\"], log_p: Float[Array, \"N\"]\n",
    "):\n",
    "    ones = jnp.ones_like(weights)[:, None]\n",
    "    w_inner_prod = lambda a, b: jnp.einsum(\"i,ij,ik->jk\", weights, a, b)\n",
    "\n",
    "    X_T_W_X = jnp.block(\n",
    "        [\n",
    "            [\n",
    "                w_inner_prod(ones, ones),\n",
    "                w_inner_prod(ones, signal),\n",
    "                w_inner_prod(ones, -0.5 * signal**2),\n",
    "            ],\n",
    "            [\n",
    "                w_inner_prod(signal, ones),\n",
    "                w_inner_prod(signal, signal),\n",
    "                w_inner_prod(signal, -0.5 * signal**2),\n",
    "            ],\n",
    "            [\n",
    "                w_inner_prod(-0.5 * signal**2, ones),\n",
    "                w_inner_prod(-0.5 * signal**2, signal),\n",
    "                w_inner_prod(-0.5 * signal**2, -0.5 * signal**2),\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "    X_T_W_y = jnp.concatenate(\n",
    "        [\n",
    "            w_inner_prod(ones, log_p[:, None]),\n",
    "            w_inner_prod(signal, log_p[:, None]),\n",
    "            w_inner_prod(-0.5 * signal**2, log_p[:, None]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    beta = jnp.linalg.solve(X_T_W_X, X_T_W_y[:, 0])\n",
    "    return beta\n",
    "\n",
    "\n",
    "def modified_efficient_importance_sampling(\n",
    "    y: Float[Array, \"n+1 p\"],  # observations\n",
    "    model: PGSSM,  # model\n",
    "    z_init: Float[Array, \"n+1 p\"],  # initial z estimate\n",
    "    Omega_init: Float[Array, \"n+1 p p\"],  # initial Omega estimate\n",
    "    n_iter: int,  # number of iterations\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random key\n",
    "    eps: Float = 1e-5,  # convergence threshold\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    z, Omega = z_init, Omega_init\n",
    "\n",
    "    np1, p, m = model.B.shape\n",
    "\n",
    "    key, crn_key = jrn.split(key)\n",
    "\n",
    "    v_norm_w = vmap(normalize_weights)\n",
    "    dist = model.dist\n",
    "    lw_t = vmap(\n",
    "        vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega)),\n",
    "        (0, None, None, None, None),\n",
    "    )\n",
    "\n",
    "    def _break(val):\n",
    "        i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        # in first iteration we don't have old values, converged is True for NaNs\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(\n",
    "            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached\n",
    "        )\n",
    "\n",
    "    def _iteration(val):\n",
    "        i, z, Omega, _, _ = val\n",
    "        glssm_approx = GLSSM(\n",
    "            model.u,\n",
    "            model.A,\n",
    "            model.D,\n",
    "            model.Sigma0,\n",
    "            model.Sigma,\n",
    "            model.v,\n",
    "            model.B,\n",
    "            Omega,\n",
    "        )\n",
    "        sim_signal = simulation_smoother(glssm_approx, z, N, crn_key)\n",
    "\n",
    "        log_weights = lw_t(sim_signal, y, model.xi, z, Omega)\n",
    "        log_p = dist(sim_signal, model.xi).log_prob(y).sum(axis=-1)\n",
    "        wls_estimate = vmap(optimal_parameters, (1, 1, 1), 0)(\n",
    "            sim_signal, v_norm_w(log_weights), log_p\n",
    "        )\n",
    "\n",
    "        a = wls_estimate[:, 0]\n",
    "        b = wls_estimate[:, 1 : (p + 1)]\n",
    "        c = wls_estimate[:, (p + 1) :]\n",
    "\n",
    "        z_new = b / c\n",
    "        Omega_new = vmap(jnp.diag)(1 / c)\n",
    "\n",
    "        return i + 1, z_new, Omega_new, z, Omega\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "\n",
    "    n_iters, z, Omega, z_old, Omega_old = while_loop(\n",
    "        _keep_going,\n",
    "        _iteration,\n",
    "        (0, z_init, Omega_init, jnp.empty_like(z_init), jnp.empty_like(Omega_init)),\n",
    "    )\n",
    "\n",
    "    proposal = GLSSMProposal(\n",
    "        u=model.u,\n",
    "        A=model.A,\n",
    "        D=model.D,\n",
    "        Sigma0=model.Sigma0,\n",
    "        Sigma=model.Sigma,\n",
    "        v=model.v,\n",
    "        B=model.B,\n",
    "        Omega=Omega,\n",
    "        z=z,\n",
    "    )\n",
    "\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "\n",
    "    return proposal, information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import log_weights\n",
    "from isssm.pgssm import simulate_pgssm, nb_pgssm_running_example\n",
    "from isssm.kalman import FFBS\n",
    "from isssm.kalman import kalman, smoother\n",
    "import jax.random as jrn\n",
    "from isssm.laplace_approximation import laplace_approximation\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from isssm.importance_sampling import ess_lw\n",
    "from isssm.typing import PGSSM\n",
    "from isssm.kalman import smoothed_signals\n",
    "from isssm.laplace_approximation import posterior_mode\n",
    "from isssm.typing import GLSSMProposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb_pgssm_running_example(\n",
    "    s_order=2, Sigma0_seasonal=jnp.eye(2 - 1), x0_seasonal=jnp.zeros(2 - 1)\n",
    ")\n",
    "key = jrn.PRNGKey(511)\n",
    "key, subkey = jrn.split(key)\n",
    "N = 1\n",
    "(x,), (y,) = simulate_pgssm(model, N, subkey)\n",
    "proposal_la, info_la = laplace_approximation(y, model, 10)\n",
    "\n",
    "N = int(1e4)\n",
    "key, subkey = jrn.split(key)\n",
    "proposal_meis, info_meis = modified_efficient_importance_sampling(\n",
    "    y, model, proposal_la.z, proposal_la.Omega, 10, N, subkey\n",
    ")\n",
    "glssm_meis = GLSSM(\n",
    "    model.u,\n",
    "    model.A,\n",
    "    model.A,\n",
    "    model.Sigma0,\n",
    "    model.Sigma,\n",
    "    model.v,\n",
    "    model.B,\n",
    "    proposal_meis.Omega,\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.tight_layout()\n",
    "\n",
    "z_diff = ((proposal_meis.z - proposal_la.z) ** 2).mean(axis=1)\n",
    "axs[0].set_title(\"Difference in z ME vs. MEIS\")\n",
    "axs[0].plot(z_diff)\n",
    "\n",
    "Omega_diff = ((proposal_meis.Omega - proposal_la.Omega) ** 2).mean(axis=(1, 2))\n",
    "axs[1].set_title(\"Difference in Omega ME vs. MEIS\")\n",
    "axs[1].plot(Omega_diff)\n",
    "\n",
    "s_smooth_la = posterior_mode(proposal_la)\n",
    "s_smooth_meis = posterior_mode(proposal_meis)\n",
    "\n",
    "axs[2].set_title(\"Smoothed signals\")\n",
    "axs[2].plot(s_smooth_la, label=\"LA\")\n",
    "axs[2].plot(s_smooth_meis, label=\"MEIS\")\n",
    "axs[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.importance_sampling import pgssm_importance_sampling\n",
    "from isssm.importance_sampling import ess_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "_, lw_la = pgssm_importance_sampling(\n",
    "    y, model, proposal_la.z, proposal_la.Omega, N, jrn.PRNGKey(423423)\n",
    ")\n",
    "samples, lw = pgssm_importance_sampling(\n",
    "    y, model, proposal_meis.z, proposal_meis.Omega, N, jrn.PRNGKey(423423)\n",
    ")\n",
    "weights = normalize_weights(lw)\n",
    "plt.title(f\"EIS weights, ESS = {ess_pct(lw):.2f}% (vs. LA ESS = {ess_pct(lw_la):.2f}%)\")\n",
    "plt.hist(4 * N * weights[None, :], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EIS increases ESS of importance sampling from LA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
