{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp laplace_approximation\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace approximation for log-concave state space models\n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=subsection.3.4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider an [PGSSM](20_pgssm.ipynb) with states $X$ and observations $Y$.\n",
    "If the joint distribution of $X$ and $Y$ is not Gaussian, we are unable to perform the standard [Kalman filter and smoother](10_kalman_filter_smoother.ipynb).\n",
    "\n",
    "Here we implement the alternative Laplace approximation (LA) method from [@Durbin2012Time], Chapter 10.6, which is called mode approximation there. It's main idea is to approximate the posterior distribution by a Gaussian distribution by a second-order Taylor expansion for the log-pdf around the posterior mode.\n",
    "\n",
    "This essentially means matching the first and second-order derivatives of the observation log-likelihoods at the mode. As the mode is a (global) maximum of the posterior distribution, we can find it by a Newton-Raphson iteration for which [@Durbin2012Time] show, that it can be implemented efficiently by a single pass of a Kalman smoother. \n",
    "\n",
    "The LA procedure is based on the observation that at the mode $\\hat s = (\\hat s_0, \\dots, \\hat s_n)$ the surrogate Gaussian model with the same state equation and observation equations\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "S_t &= B_t X_t \\\\\n",
    "Z_t &= S_t + \\eta_t \\\\\n",
    "\\eta_t &\\sim \\mathcal N\\left(0, \\Omega_t\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "for $\\Omega_t^{-1} = -\\frac{\\partial^2 \\log p(y_t|\\cdot)}{\\partial (s_t)^2}|_{\\hat s_t}$ has, for $z_t = s_t +\\Omega_t {\\partial p(y_t|\\cdot)}{\\partial s_t}|_{\\hat s_t}$ mode $\\hat s$.\n",
    "\n",
    "In most cases we are interested in, the observations are conditionally independent given the signals such that $\\Omega$ is a **diagonal matrix**, which makes inversion much faster as we only have to invert the diagonals. This implementation assumes this to hold, but could be extended to handel the general case as well (replace the calls to `vdiag` by `solve`).\n",
    "\n",
    "This is used in a fixed point iteration:\n",
    "\n",
    "1. Start with an initial guess $\\hat s$.\n",
    "2. Setup the above Gaussian approximation.\n",
    "3. Perform a pass of the signal smoother, obtaining the posterior mode $\\hat s^+$.\n",
    "4. Set $\\hat s = \\hat s^+$ and iterate until convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-attention}\n",
    "Currently, we assume that $\\Omega$ is always positive definite, i.e. that $s\\mapsto p(y|s)$ is strictly log-concave. This is the case for natural exponential families but might be violated otherwise. In this case, the Kalman filter and smoother can still be used to perform the Laplace approximation, but then has to be based on the ideas developed in [@Jungbacker2007Monte].\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-fold: true\n",
    "from functools import partial\n",
    "\n",
    "import fastcore.test as fct\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "import matplotlib.pyplot as plt\n",
    "from jax import jit, vmap\n",
    "from jax.scipy.special import expit\n",
    "\n",
    "from isssm.kalman import kalman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from functools import partial\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "from jax import grad, jacfwd, jacrev, jit, vmap\n",
    "from jax.lax import scan, while_loop\n",
    "from jaxtyping import Array, Float\n",
    "\n",
    "from isssm.kalman import kalman, smoother\n",
    "from isssm.pgssm import simulate_pgssm\n",
    "from isssm.util import converged\n",
    "from isssm.typing import GLSSM, PGSSM, InitialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# should we jit + partial the application of B in laplace_approximation?\n",
    "\n",
    "vmm = vmap(jnp.matmul)\n",
    "\n",
    "M = jrn.normal(jrn.PRNGKey(34234), (100, 200, 200))\n",
    "x = jrn.normal(jrn.PRNGKey(34234), (100, 200))\n",
    "\n",
    "vM = partial(vmm, M)\n",
    "jvM = jit(partial(vmm, M))\n",
    "jvmm = jit(vmm)\n",
    "\n",
    "# %timeit vM(x).block_until_ready()\n",
    "# %timeit jvM(x).block_until_ready()\n",
    "# %timeit vmm(M,x).block_until_ready()\n",
    "# %timeit jvmm(M,x).block_until_ready()\n",
    "# seems jvmm/jvM is the fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from isssm.kalman import smoothed_signals\n",
    "from isssm.typing import to_glssm\n",
    "from isssm.typing import GLSSMProposal, ConvergenceInformation\n",
    "from jax.scipy.optimize import minimize\n",
    "\n",
    "vmm = jit(vmap(jnp.matmul))\n",
    "vdiag = jit(vmap(jnp.diag))\n",
    "vvmap = lambda fun: vmap(vmap(fun))\n",
    "\n",
    "SmoothState = Float[Array, \"n+1 m\"]\n",
    "PseudoObs = Float[Array, \"n+1 p\"]\n",
    "PseudoObsCov = Float[Array, \"n+1 p p\"]\n",
    "\n",
    "default_link = lambda y: jnp.log(y + 1.0)\n",
    "\n",
    "\n",
    "def _initial_guess(xi_ti, y_ti, dist, link=default_link):\n",
    "    result = minimize(\n",
    "        lambda s_ti: -dist(s_ti, xi_ti).log_prob(y_ti).sum(),\n",
    "        jnp.atleast_1d(default_link(y_ti)),\n",
    "        method=\"BFGS\",\n",
    "    )\n",
    "    return jnp.squeeze(result.x)\n",
    "\n",
    "\n",
    "def laplace_approximation(\n",
    "    y: Float[Array, \"n+1 p\"],  # observation\n",
    "    model: PGSSM,\n",
    "    n_iter: int,  # number of iterations\n",
    "    log_lik=None,  # log likelihood function\n",
    "    d_log_lik=None,  # derivative of log likelihood function\n",
    "    dd_log_lik=None,  # second derivative of log likelihood function\n",
    "    eps: Float = 1e-5,  # precision of iterations\n",
    "    link=default_link,  # default link to use in initial guess\n",
    ") -> tuple[GLSSMProposal, ConvergenceInformation]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, dist, xi = model\n",
    "    np1, p, m = B.shape\n",
    "\n",
    "    s_init = vvmap(partial(_initial_guess, dist=dist, link=link))(xi, y)\n",
    "\n",
    "    def default_log_lik(s_ti, xi_ti, y_ti):\n",
    "        return dist(s_ti, xi_ti).log_prob(y_ti).sum()\n",
    "\n",
    "    if log_lik is None:\n",
    "        log_lik = default_log_lik\n",
    "\n",
    "    if d_log_lik is None:\n",
    "        d_log_lik = jacfwd(log_lik, argnums=0)\n",
    "    if dd_log_lik is None:\n",
    "        dd_log_lik = jacrev(d_log_lik, argnums=0)\n",
    "\n",
    "    vd_log_lik = jit(vvmap(d_log_lik))\n",
    "    vdd_log_lik = jit(vvmap(dd_log_lik))\n",
    "\n",
    "    def _break(val):\n",
    "        _, i, z, Omega, z_old, Omega_old = val\n",
    "\n",
    "        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)\n",
    "        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)\n",
    "        all_converged = jnp.logical_and(z_converged, Omega_converged)\n",
    "\n",
    "        iteration_limit_reached = i >= n_iter\n",
    "\n",
    "        return jnp.logical_or(all_converged, iteration_limit_reached)\n",
    "\n",
    "    def _iteration(val):\n",
    "        s, i, z_old, Omega_old, _, _ = val\n",
    "\n",
    "        grad = vd_log_lik(s, xi, y)\n",
    "        Gamma = -vdd_log_lik(s, xi, y)\n",
    "        # assume hessian is diagonal\n",
    "        Omega = vdiag(1.0 / Gamma)\n",
    "\n",
    "        z = s + grad / Gamma\n",
    "        approx_glssm = GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega)\n",
    "\n",
    "        filtered = kalman(z, approx_glssm)\n",
    "        s_new = smoothed_signals(filtered, z, approx_glssm)\n",
    "\n",
    "        return s_new, i + 1, z, Omega, z_old, Omega_old\n",
    "\n",
    "    empty_z = jnp.empty_like(s_init)\n",
    "    empty_Omega = jnp.empty((np1, p, p))\n",
    "    init = (s_init, 0, empty_z, empty_Omega, empty_z, empty_Omega)\n",
    "\n",
    "    _keep_going = lambda *args: jnp.logical_not(_break(*args))\n",
    "    _, n_iters, z, Omega, z_old, Omega_old = while_loop(_keep_going, _iteration, init)\n",
    "\n",
    "    final_proposal = GLSSMProposal(u, A, D, Sigma0, Sigma, v, B, Omega, z)\n",
    "    delta_z = jnp.max(jnp.abs(z - z_old))\n",
    "    delta_Omega = jnp.max(jnp.abs(Omega - Omega_old))\n",
    "    information = ConvergenceInformation(\n",
    "        converged=jnp.logical_and(\n",
    "            converged(z, z_old, eps), converged(Omega, Omega_old, eps)\n",
    "        ),\n",
    "        n_iter=n_iters,\n",
    "        delta=jnp.max(jnp.array([delta_z, delta_Omega])),\n",
    "    )\n",
    "    return final_proposal, information\n",
    "\n",
    "\n",
    "def posterior_mode(proposal: GLSSMProposal) -> Float[Array, \"n+1 p\"]:\n",
    "    glssm = to_glssm(proposal)\n",
    "    return smoothed_signals(kalman(proposal.z, glssm), proposal.z, glssm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return to our [running example](20_lcssm.ipynb#Running) and use mode estimation to obtain an estimate for the mode of the conditional distribution of states given the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isssm.pgssm import nb_pgssm_running_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_order = 5\n",
    "model = nb_pgssm_running_example(\n",
    "    s_order=s_order,\n",
    "    Sigma0_seasonal=jnp.eye(s_order - 1),\n",
    "    x0_seasonal=jnp.zeros(s_order - 1),\n",
    ")\n",
    "\n",
    "key = jrn.PRNGKey(511)\n",
    "key, subkey = jrn.split(key)\n",
    "N = 1\n",
    "(X,), (Y,) = simulate_pgssm(model, N, subkey)\n",
    "\n",
    "proposal, info = laplace_approximation(Y, model, 10)\n",
    "smooth_signal = posterior_mode(proposal)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.set_title(f\"Observations and signal mode after {info.n_iter} iterations\")\n",
    "ax1.plot(Y[:], linestyle=\"--\", color=\"gray\", label=\"$Y_t$\")\n",
    "ax1.plot(jnp.exp(smooth_signal), color=\"blue\", label=\"$\\\\exp (S_t) $\")\n",
    "ax2.set_title(\"true signal and mode\")\n",
    "ax2.plot(vmap(jnp.matmul)(model.B, X), linestyle=\"--\", color=\"gray\", label=\"$S_t$\")\n",
    "ax2.plot(smooth_signal, color=\"blue\", label=\"mode\")\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "# unique legend\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# by_label = dict(zip(labels, handles))\n",
    "# plt.legend(by_label.values(), by_label.keys())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default implementation of the `mode_estimation` method uses automatic differentiation to evaluate the first and second derivatives necessary to implement the LA. You can also provide the derivatives yourself, e.g. for efficiency or numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: True\n",
    "r = 20.\n",
    "def nb_log_lik(s_ti, r_ti, y_ti):\n",
    "    return jnp.sum(y_ti * jnp.log(expit(s_ti - jnp.log(r_ti))) - r_ti * jnp.log(jnp.exp(s_ti) + r_ti))\n",
    "\n",
    "def d_nb_log_lik(s_ti, r_ti, y_ti):\n",
    "    return y_ti - (y_ti + r_ti) * expit(s_ti - jnp.log(r_ti))\n",
    "\n",
    "def dd_nb_log_lik(s_ti, r_ti, y_ti):\n",
    "    return -(y_ti + r_ti) * expit(s_ti - jnp.log(r_ti)) * (1 - expit(s_ti - jnp.log(r_ti)))\n",
    "\n",
    "print(\"10 iterations of LA with AD \")\n",
    "%timeit laplace_approximation(Y, model, 10)[0][0].block_until_ready()\n",
    "print(\"10 iterations of LA with analytical gradients\")\n",
    "%timeit laplace_approximation(Y, model, 10, nb_log_lik, d_nb_log_lik, dd_nb_log_lik)[0][0].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
