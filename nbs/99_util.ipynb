{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp util\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "from jaxtyping import Array, Float, Bool\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "from tensorflow_probability.substrates.jax.distributions import (\n",
    "    MultivariateNormalLinearOperator as MVNLO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sampling from degenerate Multivariate normal\n",
    "\n",
    "The `MultivariateNormalFullCovariance` distribution from `tfp` only supports non-singular covariance matrices for sampling, because internally a Cholesky decomposition is used, which is ambiguous for singular symmetric matrices. Instead, we use an eigenvalue decomposition, and compute a valid Cholesky root by QR-decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "LOFM = tfp.tf2jax.linalg.LinearOperatorFullMatrix\n",
    "LOLT = tfp.tf2jax.linalg.LinearOperatorLowerTriangular\n",
    "\n",
    "\n",
    "def degenerate_cholesky(Sigma):\n",
    "    evals, evecs = jnp.linalg.eigh(Sigma)\n",
    "    # transpose for QR\n",
    "    # ensure positive eigenvalues\n",
    "    sqrt_cov = jnp.einsum(\"...ij,...j->...ji\", evecs, jnp.sqrt(jnp.abs(evals)))\n",
    "    Q, R = jnp.linalg.qr(sqrt_cov, mode=\"complete\")\n",
    "    # ensure positive diagonal\n",
    "    R = R * jnp.sign(jnp.einsum(\"...ii->...i\", R)[..., None])\n",
    "    L = R.swapaxes(-1, -2)\n",
    "    return L\n",
    "\n",
    "\n",
    "def MVN_degenerate(\n",
    "    loc: Array, cov: Array\n",
    ") -> tfp.distributions.MultivariateNormalLinearOperator:\n",
    "    L = degenerate_cholesky(cov)\n",
    "    return MVNLO(loc=loc, scale=LOLT(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jrn\n",
    "import matplotlib.pyplot as plt\n",
    "import fastcore.test as fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = jnp.zeros(2)\n",
    "Sigma = jnp.array([[1.0, 1.0], [1.0, 1.0]])\n",
    "\n",
    "N = 1000\n",
    "key = jrn.PRNGKey(1423423)\n",
    "key, subkey = jrn.split(key)\n",
    "samples = MVN_degenerate(mu, Sigma).sample(seed=subkey, sample_shape=(N,))\n",
    "plt.title(\"Samples from degenerate 2D Gaussian\")\n",
    "plt.scatter(samples[:, 0], samples[:, 1])\n",
    "plt.show()\n",
    "\n",
    "fct.test_close(samples @ jnp.array([[1.0], [-1.0]]), jnp.zeros(N))\n",
    "\n",
    "L = degenerate_cholesky(Sigma)\n",
    "# ensure cholesky is correct\n",
    "fct.test_close(Sigma, L @ L.T)\n",
    "fct.test_ne(Sigma, L.T @ L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def converged(\n",
    "    new: Float[Array, \"...\"],  # the new array\n",
    "    old: Float[Array, \"...\"],  # the old array\n",
    "    eps: Float,  # tolerance\n",
    ") -> Bool:  # whether the arrays are close enough\n",
    "    \"\"\"check that sup-norm of relative change is smaller than tolerance\"\"\"\n",
    "    is_close = jnp.max(jnp.abs((new - old) / old)) < eps\n",
    "    any_nans = jnp.isnan(new).sum() > 0\n",
    "    return jnp.logical_or(is_close, any_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmapped utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the package we make extensive use of matrix-vector multiplication. Depending on the algorithm, different vectorizations are helpful. \n",
    "\n",
    "Let $B \\in \\mathbf R^{(n+1)\\times p \\times m}$ be a list of $n + 1$ matrices, let $X \\in \\mathbf R^{(n + 1) \\times m}$ be a set of states and let $\\mathbf X \\in \\mathbf R^{N \\times (n + 1) \\times p}$ be $N$ simulations of $X$. \n",
    "\n",
    "`mm_sim` allows to multiply at a single time point $t$ the single matrix $B_t$ with all $X_t^i$, i.e, maps $$\\mathbf R^{p \\times m} \\times \\mathbf R^{N \\times m} \\to \\mathbf R^{N \\times p}.$$\n",
    "\n",
    "`mm_time` allows to map the single sample $X$ for each time $t$ to $(B_tX_t)_{t = 0, \\dots, n}$, i.e. maps $$\\mathbf R^{(n +1) \\times p \\times m} \\times \\mathbf R^{(n + 1) \\times m} \\to \\mathbf R^{(n+1) \\times p}.$$\n",
    "\n",
    "`mm_time_sim` allows to multiply all samples $\\mathbf X$ ;or all times with matrices $B$, i.e. maps from $$\\mathbf R^{(n + 1) \\times p \\times m}\\times \\mathbf R^{N \\times (n+1) \\times m} \\to \\mathbf R^{N \\times (n + 1) \\times p}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "# multiply $B_t$ and $X^i_t$\n",
    "mm_sim = vmap(jnp.matmul, (None, 0))\n",
    "# matmul with $(B_t)_{t}$ and $(X_t)_{t}$\n",
    "mm_time = vmap(jnp.matmul, (0, 0))\n",
    "# matmul with $(B_t)_{t}$ and $(X^i_t)_{i,t}$\n",
    "mm_time_sim = vmap(mm_time, (None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, np1, p, m = 1000, 100, 3, 5\n",
    "key, subkey = jrn.split(key)\n",
    "B = jrn.normal(subkey, (np1, p, m))\n",
    "key, subkey = jrn.split(key)\n",
    "X = jrn.normal(subkey, (N, np1, m))\n",
    "\n",
    "fct.test_eq(mm_sim(B[0], X[:, 0, :]).shape, (N, p))\n",
    "fct.test_eq(mm_time(B, X[0]).shape, (np1, p))\n",
    "fct.test_eq(mm_time_sim(B, X).shape, (N, np1, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending to the front of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def append_to_front(a0: Float[Array, \"...\"], a: Float[Array, \"n ...\"]):\n",
    "    return jnp.concatenate([a0[None], a], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antithetic variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the efficiency of importance sampling [@Durbin1997Monte] recommend using antithetic variables. These are a device to reduce Monte-Carlo variance by introducing negative correlations. We use both location- and scale-balanced antithetic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tensorflow_probability.substrates.jax.distributions import Chi2\n",
    "\n",
    "\n",
    "def location_antithetic(samples: Float[Array, \"N ...\"], mean: Float[Array, \"N ...\"]):\n",
    "    return 2 * mean[None] - samples\n",
    "\n",
    "\n",
    "def scale_antithethic(\n",
    "    u: Float[Array, \"N n+1 k\"],\n",
    "    samples: Float[Array, \"N n+1 p\"],\n",
    "    mean: Float[Array, \"n+1 p\"],\n",
    "):\n",
    "\n",
    "    N, l = u.shape\n",
    "    # ensure dtype is Float64\n",
    "    chi_dist = Chi2(l * jnp.ones(1))\n",
    "\n",
    "    c = jnp.linalg.norm(u, axis=1) ** 2\n",
    "    c_prime = chi_dist.quantile(1.0 - chi_dist.cdf(c))\n",
    "\n",
    "    return mean[None] + jnp.sqrt(c_prime / c)[:, None, None] * (samples - mean[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
