{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp kalman\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "import jax.scipy.linalg as jsla\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "from jax import vmap, jit\n",
    "from jax.lax import scan\n",
    "from jaxtyping import Array, Float, PRNGKeyArray\n",
    "\n",
    "from isssm.typing import GLSSM, FilterResult, Observations, SmootherResult\n",
    "from isssm.util import MVN_degenerate as MVN, mm_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for this notebook\n",
    "from isssm.models.stsm import stsm\n",
    "import jax\n",
    "import numpy.testing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "from isssm.glssm import simulate_glssm\n",
    "from isssm.typing import GLSSM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman filter and smoother variants in JAX\n",
    "> See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=section.3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a [GLSSM](00_glssm.ipynb) of the form\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X_0 &\\sim \\mathcal N (u_0, \\Sigma_0) &\\\\\n",
    "    X_{t + 1} &= u_{t + 1} + A_t X_{t} + \\varepsilon_{t + 1} &t = 0, \\dots, n - 1\\\\\n",
    "    \\varepsilon_t &\\sim \\mathcal N (0, \\Sigma_t) & t = 1, \\dots, n \\\\\n",
    "    Y_t &= v_{t} + B_t X_t + \\eta_t & t =0, \\dots, n & \\\\\n",
    "    \\eta_t &\\sim \\mathcal N(0, \\Omega_t), & t=0, \\dots, n.\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter\n",
    "For $t, s \\in \\{0, \\dots, n\\}$ consider the following BLPs and associated covariance matrices\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat X_{t|s} &= \\mathbf E \\left( X_t | Y_s, \\dots, Y_0\\right) \\\\\n",
    "    \\Xi_{t | s} &= \\text{Cov} \\left(X_t | Y_s, \\dots, Y_0 \\right)\\\\\n",
    "    \\hat Y_{t|s} &= \\mathbf E \\left( Y_t | Y_s, \\dots, Y_0\\right) \\\\\n",
    "    \\Psi_{t | s} &= \\text{Cov} \\left(Y_t | Y_s, \\dots, Y_0 \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kalman filter consists of the following two-step recursion:\n",
    "\n",
    "#### Initialization\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat X_{0|0} &= u_0\\\\\n",
    "\\Xi_{0|0} &= \\Sigma_0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Iterate for $t = 0, \\dots, n-1$\n",
    "\n",
    "#### Prediction\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat X_{t + 1|t} &= u_{t + 1} + A_t \\hat X_{t | t} \\\\\n",
    "    \\Xi_{t + 1 | t} &= A_t \\Xi_{t|t} A_t^T + \\Sigma_t\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Filtering\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat Y_{t + 1 | t} &= v_{t} + B_t \\hat X_{t + 1 | t} \\\\\n",
    "    \\Psi_{t + 1| t} &= B_{t + 1} \\Xi_{t + 1 | t} B_{t + 1}^T + \\Omega_{t + 1}\\\\\n",
    "    K_t &= \\Xi_{t + 1 | t} B_{t + 1}^T \\Psi_{t + 1 | t} ^{-1} \\\\\n",
    "    \\hat X_{t + 1 | t + 1} &= \\hat X_{t + 1 | t} + K_t (Y_{t + 1} - \\hat Y_{t + 1 | t})\\\\\n",
    "    \\Xi_{t + 1 | t + 1} &= \\Xi_{t + 1 | t} - K_t \\Psi_{t + 1| t} K_t^T \n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from isssm.util import append_to_front\n",
    "\n",
    "\n",
    "def _predict(\n",
    "    x_filt: Float[Array, \"m\"],  # $X_{t|t}$\n",
    "    Xi_filt: Float[Array, \"m m\"],  # $\\Xi_{t|t}\n",
    "    u: Float[Array, \"m\"],  # $u_{t + 1}$\n",
    "    A: Float[Array, \"m m\"],  # $A_t$\n",
    "    Sigma: Float[Array, \"l l\"],  # $\\Sigma_{t + 1}\n",
    "    D: Float[Array, \"m l\"],\n",
    "):\n",
    "    \"\"\"perform a single prediction step\"\"\"\n",
    "    x_pred = A @ x_filt + u\n",
    "    Xi_pred = A @ Xi_filt @ A.T + D @ Sigma @ D.T\n",
    "\n",
    "    return x_pred, Xi_pred\n",
    "\n",
    "\n",
    "def _filter(\n",
    "    x_pred: Float[Array, \"m\"],\n",
    "    Xi_pred: Float[Array, \"m m\"],\n",
    "    y: Float[Array, \"p\"],\n",
    "    v: Float[Array, \"p\"],\n",
    "    B: Float[Array, \"p m\"],\n",
    "    Omega: Float[Array, \"p p\"],\n",
    "):\n",
    "    \"\"\"perform a single filtering step\"\"\"\n",
    "    y_pred = v + B @ x_pred\n",
    "    Psi_pred = B @ Xi_pred @ B.T + Omega\n",
    "    K = (\n",
    "        Xi_pred @ B.T @ jnp.linalg.pinv(Psi_pred, hermitian=True)\n",
    "    )  # jsla.solve(Psi_pred, B).T\n",
    "    x_filt = x_pred + K @ (y - y_pred)\n",
    "    Xi_filt = Xi_pred - K @ Psi_pred @ K.T\n",
    "\n",
    "    return x_filt, Xi_filt\n",
    "\n",
    "\n",
    "def kalman(\n",
    "    y: Observations,  # observatoins\n",
    "    glssm: GLSSM,  # model\n",
    ") -> FilterResult:  # filtered & predicted states and covariances\n",
    "    \"\"\"Perform the Kalman filter\"\"\"\n",
    "    u, A, D, Sigma0, Sigma, v, B, Omega = glssm\n",
    "    np1, p, m = B.shape\n",
    "    _, m, l = D.shape\n",
    "\n",
    "    def step(carry, inputs):\n",
    "        x_filt, Xi_filt = carry\n",
    "        y, u, A, D, Sigma, v, B, Omega = inputs\n",
    "\n",
    "        x_pred, Xi_pred = _predict(x_filt, Xi_filt, u, A, Sigma, D)\n",
    "        x_filt_next, Xi_filt_next = _filter(x_pred, Xi_pred, y, v, B, Omega)\n",
    "\n",
    "        return (x_filt_next, Xi_filt_next), (x_filt_next, Xi_filt_next, x_pred, Xi_pred)\n",
    "\n",
    "    # artificial state X_{-1} with mean x_0\n",
    "    # covariance Sigma_0, transition identity and innovation cov 0\n",
    "    # will lead to X_0 having correct predictive distribution\n",
    "    # this avoids having to compute a separate filtering step beforehand\n",
    "\n",
    "    A_ext = append_to_front(jnp.eye(m), A)\n",
    "    Sigma_ext = append_to_front(jnp.zeros((l, l)), Sigma)\n",
    "    D_ext = append_to_front(jnp.zeros((m, l)), D)\n",
    "    u_ext = append_to_front(jnp.zeros(m), u[1:])\n",
    "\n",
    "    init = (u[0], Sigma0)\n",
    "\n",
    "    _, (x_filt, Xi_filt, x_pred, Xi_pred) = scan(\n",
    "        step, init, (y, u_ext, A_ext, D_ext, Sigma_ext, v, B, Omega)\n",
    "    )\n",
    "\n",
    "    return FilterResult(x_filt, Xi_filt, x_pred, Xi_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that our implementation works as expected by simulating a single sample from the joint distribution of a [structural time series model](models/10_stsm.ipynb) with seasonality of order 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glssm_model = stsm(jnp.ones(3), 0.0, 0.1, 0.1, 100, jnp.eye(3), 3, 2)\n",
    "\n",
    "key = jrn.PRNGKey(53405234)\n",
    "key, subkey = jrn.split(key)\n",
    "(x,), (y,) = simulate_glssm(glssm_model, 1, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_filt, Xi_filt, x_pred, Xi_pred = kalman(y, glssm_model)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.set_title(\"\")\n",
    "ax1.plot(y, label=\"$Y$\")\n",
    "ax1.plot(x[:, 0], label=\"$X$\")\n",
    "ax1.plot(x_filt[:, 0], label=\"$\\\\hat X_{{t|t}}$\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x[:, 2:])\n",
    "ax2.set_title(\"Seasonal component $X_{{t, (2,3)}}$\")\n",
    "\n",
    "ax3.set_title(\"Filtered seasonal component $\\\\hat X_{{t, (2,3)|t}}$\")\n",
    "ax3.plot(x_filt[:, 2:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us hasten to add that there is no reason to believe that $X_{t}$ and $\\hat X_{t|n}$ should be close. Nevertheless, we can use this comparison as a sanity check whether our implementation gives reasonable estimates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman smoother\n",
    "\n",
    "The **Kalman smoother** uses the filter result to obtain $\\hat X_{t | n}$ and $\\Xi_{t | n}$ for $t = 0, \\dots n$.\n",
    "\n",
    "It is based on the following recursion with initialisation by the filtering result $\\hat X_{n | n}$ and $\\Xi_{n|n}$ and the (reverse) **gain** $G_t$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    G_t &= \\Xi_{t | t} A_t \\Xi_{t + 1 | t} ^{-1}\\\\\n",
    "    \\hat X_{t | n} &= \\hat X_{t | t} + G_t (\\hat X_{t + 1| n} - \\hat X_{t + 1 | t}) \\\\\n",
    "    \\Xi_{t | n} &= \\Xi_{t | t} - G_t (\\Xi_{t + 1 | t} - \\Xi_{t + 1 | n}) G_t^T\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "State = Float[Array, \"m\"]\n",
    "StateCov = Float[Array, \"m m\"]\n",
    "StateTransition = Float[Array, \"m m\"]\n",
    "\n",
    "\n",
    "def _smooth_step(\n",
    "    x_filt: State,\n",
    "    x_pred_next: State,\n",
    "    x_smooth_next: State,\n",
    "    Xi_filt: StateCov,\n",
    "    Xi_pred_next: StateCov,\n",
    "    Xi_smooth_next: StateCov,\n",
    "    A: StateTransition,\n",
    "):\n",
    "    err = x_smooth_next - x_pred_next\n",
    "    Gain = Xi_filt @ A.T @ jnp.linalg.pinv(Xi_pred_next, hermitian=True)\n",
    "\n",
    "    x_smooth = x_filt + Gain @ err\n",
    "    Xi_smooth = Xi_filt - Gain @ (Xi_pred_next - Xi_smooth_next) @ Gain.T\n",
    "\n",
    "    return (x_smooth, Xi_smooth)\n",
    "\n",
    "\n",
    "def smoother(\n",
    "    filter_result: FilterResult, A: Float[Array, \"n m m\"]  # transition matrices\n",
    ") -> SmootherResult:\n",
    "    \"\"\"perform the Kalman smoother\"\"\"\n",
    "    x_filt, Xi_filt, x_pred, Xi_pred = filter_result\n",
    "\n",
    "    def step(carry, inputs):\n",
    "        x_smooth_next, Xi_smooth_next = carry\n",
    "        x_filt, Xi_filt, x_pred_next, Xi_pred_next, A = inputs\n",
    "\n",
    "        x_smooth, Xi_smooth = _smooth_step(\n",
    "            x_filt, x_pred_next, x_smooth_next, Xi_filt, Xi_pred_next, Xi_smooth_next, A\n",
    "        )\n",
    "\n",
    "        return (x_smooth, Xi_smooth), (x_smooth, Xi_smooth)\n",
    "\n",
    "    _, (x_smooth, Xi_smooth) = scan(\n",
    "        step,\n",
    "        (x_filt[-1], Xi_filt[-1]),\n",
    "        (x_filt[:-1], Xi_filt[:-1], x_pred[1:], Xi_pred[1:], A),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    x_smooth = jnp.concatenate([x_smooth, x_filt[None, -1]])\n",
    "    Xi_smooth = jnp.concatenate([Xi_smooth, Xi_filt[None, -1]])\n",
    "\n",
    "    return SmootherResult(x_smooth, Xi_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply the Kalman smoother to our simulated observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = kalman(y, glssm_model)\n",
    "\n",
    "x_smooth, Xi_smooth = smoother(filtered, glssm_model.A)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "fig.tight_layout()\n",
    "ax1.set_title(\"filter and smoother\")\n",
    "ax1.plot(x_filt[:, 0], label=\"$\\\\hat X_{{t | t}} $\")\n",
    "ax1.plot(x_smooth[:, 0], label=\"$\\\\hat X_{{t | n}}$\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title(\"Smoothed seasonal components\")\n",
    "ax2.plot(x_smooth[:, 2])\n",
    "ax2.plot(x[:, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothed states are indeed smoother."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing observations\n",
    "\n",
    "When entries of $Y_{t}$ are missing, we can adapt the model by updating both $A_{t}$ and $\\Omega_{t}$. Let $$M_{t} = \\operatorname{diag} \\left( \\mathbf 1_{Y_{t, 1} \\text{ observed}}, \\dots, \\mathbf 1_{Y_{t, p} \\text{ observed}}\\right),$$ then we replace $A_{t}$ by $M_{t}A_{t}$ and $\\Omega_{t} = M_{t}\\Omega_{t}M_{t}^{T}$, see also section 4.10 in [@Durbin2012Time].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# y not jittable: boolean indices have to be concrete\n",
    "def account_for_nans(model: GLSSM, y: Observations) -> tuple[GLSSM, Observations]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, Omega = model\n",
    "\n",
    "    missing_indices = jnp.isnan(y)\n",
    "\n",
    "    y = jnp.nan_to_num(y, nan=0.0)\n",
    "\n",
    "    v = v.at[missing_indices].set(0.0)\n",
    "    B = B.at[missing_indices].set(0.0)\n",
    "    # set rows and columns of Omega to 0.\n",
    "    Omega = Omega.at[missing_indices].set(0.0)\n",
    "    Omega = Omega.transpose((0, 2, 1)).at[missing_indices].set(0.0)\n",
    "\n",
    "    return GLSSM(u, A, D, Sigma0, Sigma, v, B, Omega), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try removing some observations in the middle. Notice that the filter essentially keeps the filtered states, only applying the systems dynamics to propagate the current best estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_missing = y.at[40:60].set(jnp.nan)\n",
    "model_missing, y_accounted = account_for_nans(glssm_model, y_missing)\n",
    "filter_result_missing = kalman(y_accounted, model_missing)\n",
    "x_filt_missing, _, _, _ = filter_result_missing\n",
    "\n",
    "x_smooth_missing, _ = smoother(filter_result_missing, model_missing.A)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9, 3))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.set_title(\"\")\n",
    "ax1.plot(y_missing, label=\"$Y$\")\n",
    "ax1.plot(x[:, 0], label=\"$X$\")\n",
    "ax1.plot(x_filt_missing[:, 0], label=\"$\\\\hat X_{{t|t}}$\")\n",
    "ax1.plot(x_smooth_missing[:, 0], label=\"$\\\\hat X_{{t|n}}$\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x[:, 2:])\n",
    "ax2.set_title(\"Seasonal component $X_{{t, (2,3)}}$\")\n",
    "\n",
    "ax3.set_title(\"Filtered seasonal component $\\\\hat X_{{t, (2,3)|t}}$\")\n",
    "ax3.plot(x_filt_missing[:, 2:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction intervals\n",
    "\n",
    "As the conditional distribution of states given observations is Gaussian, we can obtain marginal prediction intervals, i.e. for every individual state $X_{t, i}$, $t = 0, \\dots, n$, $i = 1, \\dots, m$, by using the Gaussian inverse CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from tensorflow_probability.substrates.jax.distributions import Normal\n",
    "\n",
    "\n",
    "def filter_intervals(\n",
    "    result: FilterResult, alpha: Float = 0.05\n",
    ") -> Float[Array, \"2 n+1 m\"]:\n",
    "    x_filt, Xi_filt, *_ = result\n",
    "    marginal_variances = vmap(jnp.diag)(Xi_filt)\n",
    "    dist = Normal(x_filt, marginal_variances)\n",
    "    lower = dist.quantile(alpha / 2)\n",
    "    upper = dist.quantile(1 - alpha / 2)\n",
    "\n",
    "    return jnp.concatenate((lower[None], upper[None]))\n",
    "\n",
    "\n",
    "def smoother_intervals(\n",
    "    result: SmootherResult, alpha: Float = 0.05\n",
    ") -> Float[Array, \"2 n+1 m\"]:\n",
    "    x_smooth, Xi_smooth = result\n",
    "    marginal_variances = vmap(jnp.diag)(Xi_smooth)\n",
    "    dist = Normal(x_smooth, marginal_variances)\n",
    "    lower = dist.quantile(alpha / 2)\n",
    "    upper = dist.quantile(1 - alpha / 2)\n",
    "\n",
    "    return jnp.concatenate((lower[None], upper[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = kalman(y, glssm_model)\n",
    "s_result = smoother(filtered, glssm_model.A)\n",
    "\n",
    "s_lower, s_upper = smoother_intervals(s_result)\n",
    "f_lower, f_upper = filter_intervals(filtered)\n",
    "\n",
    "x_smooth, _ = s_result\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(6, 3))\n",
    "fig.tight_layout()\n",
    "ax1.set_title(\"Filtering and smoothing intervals\")\n",
    "ax1.plot(x[:20, 0], label=\"$X_{t,1}$\")\n",
    "ax1.plot(s_lower[:20, 0], linestyle=\"--\", color=\"grey\")\n",
    "ax1.plot(s_upper[:20, 0], linestyle=\"--\", color=\"grey\", label=\"95% smoothing PI\")\n",
    "ax1.plot(f_lower[:20, 0], linestyle=\"--\", color=\"orange\")\n",
    "ax1.plot(f_upper[:20, 0], linestyle=\"--\", color=\"orange\", label=\"95% filtering PI\")\n",
    "ax1.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the smoothing distribution\n",
    "\n",
    "After having run the Kalman filter we can use a recursion due to FrÃ¼hwirth-Schnatter [@Fruhwirth-Schnatter1994Data] to obtain samples from the joint conditional distribution the states given observations.\n",
    "\n",
    "By the dependency structure of states and observations the conditional densities can be factorized in the following way:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(x_0, \\dots, x_n | y_0, \\dots, y_n) &=  p(x_n | y_0, \\dots, y_n) \\prod_{t = n - 1}^0 p(x_{t}| x_{t + 1}, \\dots, x_n, y_0, \\dots, y_n) \\\\\n",
    "&= p(x_n | y_0, \\dots, y_n) \\prod_{t = n - 1}^0 p(x_{t}| x_{t + 1}, y_0, \\dots, y_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and the conditional distributions are again gaussian with conditional expecatation \n",
    "$$\n",
    "\\mathbf E (X_{t} | X_{t + 1}, Y_0, \\dots, Y_n) = \\hat X_{t|t} + G_t (X_{t + 1} - \\hat X_{t + 1|t})\n",
    "$$ and conditional covariance matrix \n",
    "$$\n",
    "\\text{Cov} (X_t | X_{t + 1}, Y_0, \\dots, Y_n) = \\Xi_{t|t} - G_t\\Xi_{t + 1 | t} G_t^T\n",
    "$$\n",
    "\n",
    "where $G_t = \\Xi_{t|t} A_t^T \\Xi_{t + 1|t}^{-1}$ is the smoothing gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _simulate_smoothed_FW1994(\n",
    "    x_filt: Float[Array, \"n+1 m\"],\n",
    "    Xi_filt: Float[Array, \"n+1 m m\"],\n",
    "    Xi_pred: Float[Array, \"n+1 m m\"],\n",
    "    A: Float[Array, \"n m m\"],\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # the random states\n",
    ") -> Float[Array, \"N n+1 m\"]:  # array of N samples from the smoothing distribution\n",
    "    r\"\"\"Simulate from smoothing distribution $p(X_0, \\dots, X_n|Y_0, \\dots, Y_n)$\"\"\"\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    X_n = MVN(x_filt[-1], Xi_filt[-1]).sample(N, subkey)\n",
    "\n",
    "    def sample_backwards(carry, inputs):\n",
    "        X_smooth_next, key = carry\n",
    "        x_filt, Xi_filt, Xi_pred, A = inputs\n",
    "\n",
    "        G = Xi_filt @ jnp.linalg.solve(Xi_pred, A).T\n",
    "\n",
    "        cond_expectation = x_filt + mm_sim(G, X_smooth_next - (A @ x_filt)[None])\n",
    "        cond_covariance = Xi_filt - G @ Xi_pred @ G.T\n",
    "\n",
    "        key, subkey = jrn.split(key)\n",
    "        new_samples = MVN(cond_expectation, cond_covariance).sample(seed=subkey)\n",
    "        return (new_samples, key), new_samples\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    _, X = scan(\n",
    "        sample_backwards,\n",
    "        (X_n, subkey),\n",
    "        (x_filt[:-1], Xi_filt[:-1], Xi_pred[1:], A),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    X_full = jnp.concatenate((X, X_n[None]))\n",
    "\n",
    "    return X_full.transpose((1, 0, 2))\n",
    "\n",
    "\n",
    "def FFBS(\n",
    "    y: Observations,  # Observations $y$\n",
    "    model: GLSSM,  # GLSSM\n",
    "    N: int,  # number of samples\n",
    "    key: PRNGKeyArray,  # random state\n",
    ") -> Float[Array, \"N n+1 m\"]:  # N samples from the smoothing distribution\n",
    "    r\"\"\"The Forward-Filter Backwards-Sampling Algorithm from [@Fruhwirth-Schnatter1994Data].\"\"\"\n",
    "    x_filt, Xi_filt, _, Xi_pred = kalman(y, model)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    return _simulate_smoothed_FW1994(x_filt, Xi_filt, Xi_pred, model.A, N, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jrn.split(key)\n",
    "(X_sim,) = FFBS(y, glssm_model, 1, subkey)\n",
    "\n",
    "assert X_sim.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_smooth, Xi_smooth = smoother(filtered, glssm_model.A)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.tight_layout()\n",
    "ax1.set_title(\"Smoothed states\")\n",
    "ax1.plot(x_smooth)\n",
    "ax2.set_title(\"Simulation from smoothing distribution\")\n",
    "ax2.plot(X_sim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# sanity check: do marginal mean of simulation coincide with Kalman smoother?\n",
    "key, subkey = jrn.split(key)\n",
    "N = 1e5\n",
    "X_sim = _simulate_smoothed_FW1994(x_filt, Xi_filt, Xi_pred, glssm_model.A, N, subkey)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "np1, p, m = glssm_model.B.shape\n",
    "for i in range(m):\n",
    "    axs[i].set_title(\"$X_{t, i}$\")\n",
    "    smoothed_vars = Xi_smooth[:, i, i]\n",
    "    ci_bounds = 2 * jnp.sqrt(smoothed_vars) / jnp.sqrt(N)\n",
    "    axs[i].plot(ci_bounds, color=\"black\", alpha=0.5, label=\"95% CI\")\n",
    "    axs[i].plot(-ci_bounds, color=\"black\", alpha=0.5)\n",
    "    axs[i].scatter(jnp.arange(np1), X_sim[:, :, i].mean(axis=0) - x_smooth[:, i])\n",
    "    axs[i].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The disturbance smoother\n",
    "\n",
    "When the interest lies in the signal $S_{t} = B_{t}X_{t}$, $t = 0, \\dots, n$, it is often more efficient to perform the following disturbance smoother, see Section 4.5 in [@Durbin2012Time] for details.\n",
    "The recursions run from $t = n$ to $t = 0$ and are initialized by $r_n =0 \\in \\R^{m}$. While it is also possible to obtain smoothed state innovations $\\hat \\varepsilon_{t | n}$, we will not be interested in them in the following, so we skip them.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat\\eta_{t | n} &= \\Omega_{t} \\left( \\Psi_{t| t - 1}^{-1}(Y_{t} - Y_{t | t - 1}) - K_{t}^{T}A_{t}^{T}r_{t} \\right) \\\\\n",
    "    L_{t} &= A_{t} \\left( I - K_{t}B_{t} \\right) \\\\\n",
    "    r_{t - 1} &= B_{t}^T \\Psi_{t | t - 1}\\left( Y_{t} - \\hat Y_{t| t - 1} \\right) + L_{t}^{T}r_{t} \n",
    "\\end{align*}\n",
    "$$\n",
    "While it is also possible to derive smoothed covariance matrices, we will not need them, as we can use [the simulation smoother](#the-simulation-smoother), which is based on mean adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.util import mm_time\n",
    "\n",
    "\n",
    "def disturbance_smoother(\n",
    "    filtered: FilterResult,  # filter result\n",
    "    y: Observations,  # observations\n",
    "    model: GLSSM,  # model\n",
    ") -> Float[Array, \"n+1 p\"]:  # smoothed disturbances\n",
    "    \"\"\"perform the disturbance smoother for observation disturbances only\"\"\"\n",
    "    x_filt, Xi_filt, x_pred, Xi_pred = filtered\n",
    "    u, A, D, Sigma0, Sigma, v, B, Omega = model\n",
    "    np1, p, m = B.shape\n",
    "\n",
    "    def step(carry, inputs):\n",
    "        (r,) = carry\n",
    "        O_Pinv_y, O_KT_AT, BT_Pinv_y, L = inputs\n",
    "\n",
    "        eta_smooth = O_Pinv_y - O_KT_AT @ r\n",
    "        r_prev = BT_Pinv_y + L.T @ r\n",
    "\n",
    "        return (r_prev,), eta_smooth\n",
    "\n",
    "    A_ext = jnp.concatenate((A, jnp.eye(m)[jnp.newaxis]), axis=0)\n",
    "    BT = B.transpose((0, 2, 1))\n",
    "\n",
    "    # offline computation is faster\n",
    "    y_tilde = y - mm_time(B, x_pred)\n",
    "    Psi_pred = B @ Xi_pred @ BT + Omega\n",
    "    Psi_pred_pinv = jnp.linalg.pinv(Psi_pred, hermitian=True)\n",
    "    O_Pinv_y = mm_time(Omega @ Psi_pred_pinv, y_tilde)\n",
    "    K = Xi_pred @ BT @ Psi_pred_pinv\n",
    "\n",
    "    KT = K.transpose((0, 2, 1))\n",
    "    AT = A_ext.transpose((0, 2, 1))\n",
    "    O_KT_AT = Omega @ KT @ AT\n",
    "    BT_Pinv_y = mm_time(BT @ Psi_pred_pinv, y_tilde)\n",
    "\n",
    "    L = A_ext @ (jnp.eye(m)[None] - K @ B)\n",
    "\n",
    "    _, eta_smooth = scan(\n",
    "        step, (jnp.zeros(m),), (O_Pinv_y, O_KT_AT, BT_Pinv_y, L), reverse=True\n",
    "    )\n",
    "\n",
    "    return eta_smooth\n",
    "\n",
    "\n",
    "def smoothed_signals(\n",
    "    filtered: FilterResult,  # filter result\n",
    "    y: Observations,  # observations\n",
    "    model: GLSSM,  # model\n",
    ") -> Float[Array, \"n+1 m\"]:  # smoothed signals\n",
    "    \"\"\"compute smoothed signals from filter result\"\"\"\n",
    "    eta_smooth = disturbance_smoother(filtered, y, model)\n",
    "    return y - eta_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_smooth_ks = vmap(jnp.matmul)(glssm_model.B, x_smooth)\n",
    "s_smooth = smoothed_signals(filtered, y, glssm_model)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.tight_layout()\n",
    "ax1.set_title(\"kalman smoother\")\n",
    "ax1.plot(s_smooth_ks)\n",
    "ax2.set_title(\"disturbance smoother\")\n",
    "ax2.plot(s_smooth)\n",
    "ax3.set_title(\"difference\")\n",
    "ax3.plot(s_smooth_ks - s_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmm = vmap(jnp.matmul)\n",
    "s_big = 100\n",
    "big_model = stsm(jnp.zeros(2 + s_big - 1), 0., .1, .1, 100, jnp.eye(2 + s_big - 1), 3, s_big)\n",
    "key, subkey = jrn.split(key)\n",
    "_, (big_y,) = simulate_glssm(big_model, 1, subkey)\n",
    "big_filtered = kalman(y, big_model)\n",
    "\n",
    "%timeit smoothed_signals(big_filtered, big_y, big_model).block_until_ready()\n",
    "%timeit vmm(big_model.B, smoother(big_filtered, big_model.A)[0]).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for large state space, but low dimensional signal, the signal smoother drastically outperforms the simple smoother, and should be preferred if our main interest lies in the signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simulation smoother\n",
    "The simulation smoother [@Durbin2002Simple] is a method for sampling from the smoothing distribution, without explicitly calculating conditional covariance matrices. It is based on the disturbance smoother. We will implement it for the signal only. \n",
    "\n",
    "1. Calculate the conditional expectation $\\mathbf E \\left( \\eta_{t} | Y_{0} = y_{0}, \\dots, Y_{n} = y_{n} \\right)$ by the disturbance smoother.\n",
    "2. Generate a new draw $(X^+, Y^+)$ from the state space model using innovations $\\eta^+$.\n",
    "3. Calculate the conditional expectation $\\mathbf E \\left( \\eta^{+} | Y_{0} = y_{0}^+, \\dots, Y_{n} = y_{n}^+\\right)$ by the disturbance smoother.\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\mathbf E \\left( \\eta_{t} | Y_{0} = y_{0}, \\dots, Y_{n} = y_{n} \\right)+ \\left(\\eta^{+} - \\mathbf E \\left( \\eta^{+} | Y_{0} = y_{0}^+, \\dots, Y_{n} = y_{n}^+\\right)\\right) \n",
    "$$ \n",
    "is a draw from the smoothing distribution $\\eta | Y_{0} = y_{0}, \\dots, Y_{n} = y_{n}$, because the second term is centered and independent from the first term. The first term contributes the mean, the second term the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tensorflow_probability.substrates.jax.distributions import Chi2\n",
    "from isssm.util import degenerate_cholesky\n",
    "from isssm.util import location_antithetic, scale_antithethic\n",
    "\n",
    "\n",
    "def _sim_from_innovations_disturbances(\n",
    "    model: GLSSM,\n",
    "    x0: Float[Array, \"N m\"],\n",
    "    eps: Float[Array, \"N n l\"],\n",
    "    eta: Float[Array, \"N n+1 p\"],\n",
    ") -> Float[Array, \"N n+1 p\"]:\n",
    "    u, A, D, Sigma0, Sigma, v, B, Omega = model\n",
    "    N, np1, l = eps.shape\n",
    "    n, m, l = D.shape\n",
    "\n",
    "    def step(carry, inputs):\n",
    "        (x,) = carry\n",
    "        u, A, D, eps, v, B, eta = inputs\n",
    "\n",
    "        x_next = u + (A @ x[..., None])[..., 0] + mm_sim(D, eps)\n",
    "        y_next = v + (B @ x[..., None])[..., 0] + eta\n",
    "\n",
    "        return (x_next,), y_next\n",
    "\n",
    "    A_ext = append_to_front(jnp.eye(m), A)\n",
    "    u_ext = append_to_front(jnp.zeros(m), u[1:])\n",
    "    D_ext = append_to_front(jnp.zeros((m, l)), D)\n",
    "    eps_ext = append_to_front(jnp.zeros((N, l)), eps.transpose((1, 0, 2)))\n",
    "\n",
    "    initial = (x0,)  # initial state\n",
    "    (x,), y = scan(\n",
    "        step,\n",
    "        initial,\n",
    "        (u_ext, A_ext, D_ext, eps_ext, v, B, eta.transpose(1, 0, 2)),\n",
    "    )\n",
    "\n",
    "    return y.transpose((1, 0, 2))\n",
    "\n",
    "\n",
    "def simulation_smoother(\n",
    "    model: GLSSM,  # model\n",
    "    y: Observations,  # observations\n",
    "    N: int,  # number of samples to draw\n",
    "    key: PRNGKeyArray,  # random number seed\n",
    ") -> Float[Array, \"N n+1 m\"]:  # N samples from the smoothing distribution of signals\n",
    "    \"\"\"Simulate from the smoothing distribution of signals\"\"\"\n",
    "    np1, p, m = model.B.shape\n",
    "    n, _, l = model.D.shape\n",
    "\n",
    "    @jit\n",
    "    def signal_filter_smoother(y, model):\n",
    "        return smoothed_signals(kalman(y, model), y, model)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    u_x0 = MVN(jnp.zeros(m), jnp.eye(m)).sample(N, subkey)\n",
    "    chol_Sigma0 = degenerate_cholesky(model.Sigma0)\n",
    "    x0 = mm_sim(chol_Sigma0, u_x0)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    u_eps = MVN(jnp.zeros((n, l)), jnp.eye(l)[None]).sample(N, subkey)\n",
    "    chol_Sigma = degenerate_cholesky(model.Sigma)\n",
    "    eps = vmap(vmap(jnp.matmul), (None, 0))(chol_Sigma, u_eps)\n",
    "\n",
    "    key, subkey = jrn.split(key)\n",
    "    u_eta = MVN(jnp.zeros((np1, p)), jnp.eye(p)[None]).sample(N, subkey)\n",
    "    chol_Omega = degenerate_cholesky(model.Omega)\n",
    "    eta = vmap(vmap(jnp.matmul), (None, 0))(chol_Omega, u_eta)\n",
    "\n",
    "    y_sim = _sim_from_innovations_disturbances(model, x0, eps, eta)\n",
    "\n",
    "    signals_smooth = signal_filter_smoother(y, model)\n",
    "    sim_signals = y_sim - eta\n",
    "    sim_signals_smooth = vmap(signal_filter_smoother, (0, None))(y_sim, model)\n",
    "\n",
    "    samples = signals_smooth[None] + (sim_signals - sim_signals_smooth)\n",
    "\n",
    "    # u = jnp.concatenate((u_x0[:, None, :], u_eps, u_eta), axis=-1)\n",
    "    u = jnp.concatenate(\n",
    "        (u_x0.reshape((N, -1)), u_eps.reshape((N, -1)), u_eta.reshape((N, -1))), axis=1\n",
    "    )\n",
    "\n",
    "    l_samples = location_antithetic(samples, signals_smooth)\n",
    "    s_samples = scale_antithethic(u, samples, signals_smooth)\n",
    "    ls_samples = scale_antithethic(u, l_samples, signals_smooth)\n",
    "\n",
    "    full_samples = jnp.concatenate((samples, l_samples, s_samples, ls_samples), axis=0)\n",
    "\n",
    "    return full_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_signals_sim = simulation_smoother(glssm_model, y, 10, subkey)\n",
    "\n",
    "signal_vars = vmap(lambda B, Xi: B @ Xi @ B.T)(glssm_model.B, Xi_smooth)[:, 0, 0]\n",
    "plt.plot(2 * signal_vars, color=\"black\", label=\"95% PI\")\n",
    "plt.title(\"Residuals in smoothed signals w/ 95% marginal PI\")\n",
    "plt.plot(-2 * signal_vars, color=\"black\")\n",
    "plt.plot(smooth_signals_sim[:, :, 0].T - s_smooth, alpha=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmm = vmap(vmap(jnp.matmul), (None, 0))\n",
    "s_big = 300\n",
    "big_model = stsm(jnp.zeros(2 + s_big - 1), 0., .1, .1, 100, jnp.eye(2 + s_big - 1), 3, s_big)\n",
    "key, subkey = jrn.split(key)\n",
    "_, (big_y,) = simulate_glssm(big_model, 1, subkey)\n",
    "N = 100\n",
    "key, subkey = jrn.split(key)\n",
    "\n",
    "# ignore antithetics, could also be used for FFBS\n",
    "%timeit simulation_smoother(big_model, big_y, N, subkey).block_until_ready()\n",
    "%timeit vmm(big_model.B, FFBS(big_y, big_model, N, subkey)).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering states from signals\n",
    "\n",
    "Both the signal and simulation smoother operate on the signals. If we are interested in the states, we have to recover them from the signals. As \n",
    "\n",
    "$$\n",
    "    S_{t} = B_{t}X_{t}\n",
    "$$\n",
    "\n",
    "for all $t$, we can recover the mode (which is the mean in the Gaussian case) of the states by performing the Kalman-filter and smoother for the signal model, i.e. where we set $\\Omega_t = \\mathbf 0_{p\\times p}$ and $y_t = s_t$. As the joint distribution of $(X,S)$ is Gaussian, the Kalman filter and smoother compute the conditional distribution $X | S$, which is Gaussian again and its mean coincides with its mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from isssm.typing import PGSSM\n",
    "\n",
    "\n",
    "def to_signal_model(model: GLSSM | PGSSM):\n",
    "    np1, p, _ = model.B.shape\n",
    "    return GLSSM(\n",
    "        model.u,\n",
    "        model.A,\n",
    "        model.D,\n",
    "        model.Sigma0,\n",
    "        model.Sigma,\n",
    "        model.v,\n",
    "        model.B,\n",
    "        jnp.zeros((np1, p, p)),\n",
    "    )\n",
    "\n",
    "\n",
    "def state_conditional_on_signal(\n",
    "    model: GLSSM | PGSSM, signal_mode: Float[Array, \"n+1 p\"]\n",
    "):\n",
    "    signal_model = to_signal_model(model)\n",
    "    return smoother(kalman(signal_mode, signal_model), signal_model.A)\n",
    "\n",
    "\n",
    "def state_mode(model: GLSSM | PGSSM, signal_mode: Float[Array, \"n+1 p\"]):\n",
    "    return state_conditional_on_signal(model, signal_mode).x_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
