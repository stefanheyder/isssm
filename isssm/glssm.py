# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_glssm.ipynb.

# %% auto 0
__all__ = ['vmatmul', 'simulate_states', 'simulate_glssm', 'simulate_smoothed_FW1994', 'FFBS', 'log_probs_x', 'log_probs_y',
           'log_prob']

# %% ../nbs/00_glssm.ipynb 4
import jax.numpy as jnp
import jax.random as jrn
from jax import vmap
from jaxtyping import Float, Array, PRNGKeyArray
from jax.lax import scan
from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN

# %% ../nbs/00_glssm.ipynb 5
vmatmul = vmap(jnp.matmul, (None, 0))


def simulate_states(
    x0: Float[Array, "m"],
    A: Float[Array, "n m m"],
    Sigma: Float[Array, "n+1 m m"],
    N: int,
    key: PRNGKeyArray
) -> Float[Array, "N n+1 m"]: 
    """Simulate states of a GLSSM

    Parameters
    ----------
    x0 : 
        initial mean $\\mathbf E \\left( X_0 \\right)$
    A : 
        transition matrices $A_t$, $t = 0, \\dots, n-1$
    Sigma : 
        covariance matrices of innovations $\\Sigma_t$, $t = 0, \\dots, n$
    N : 
        number of samples to draw
    key : PRNGKeyArray
        the random state

    Returns
    -------
    X
        an array of N samples from the state distribution 
    """

    def sim_next_states(carry, inputs):
        x_prev, key = carry
        A, Sigma = inputs

        next_loc = vmatmul(A, x_prev)
        key, subkey = jrn.split(key)

        samples = MVN(next_loc, Sigma).sample(seed=subkey)

        return (samples, key), samples

    (m,) = x0.shape
    A_ext = jnp.concatenate((jnp.eye(m)[jnp.newaxis], A))

    x0_recast = jnp.broadcast_to(x0, (N, m))
    key, subkey = jrn.split(key)
    _, X = scan(sim_next_states, (x0_recast, subkey), (A_ext, Sigma))

    return X.transpose((1, 0, 2))

# %% ../nbs/00_glssm.ipynb 6
def simulate_glssm(
    x0: Float[Array, "m"],
    A: Float[Array, "n m m"],
    B: Float[Array, "n+1 p m"],
    Sigma: Float[Array, "n+1 m m"],
    Omega: Float[Array, "n+1 p p"],
    N: int,
    key: PRNGKeyArray
) -> (Float[Array, "N n+1 m"], Float[Array, "N n+1 p"]):
    r"""Simulate states and observations of a GLSSM

    Parameters
    ----------
    x0 : 
        initial mean $\mathbf E X_0$
    A : 
        transition matrices $A_t$, $t = 0, \dots, n - 1$
    B : 
        observation matrices $B_t$, $t = 0, \dots, n$
    Sigma : 
        covariance matrices of innovations $\Sigma_t$, $t = 0, \dots, n$
    Omega : 
        covariance matrices of errors $\Omega_t$, $t=0, \dots, n$
    N : int
        number of sample paths
    key : PRNGKeyArray
        the random state

    Returns
    -------
    X, Y : 
        a tuple of two arrays each with of N samples from the state/observation distribution
    """
    key, subkey = jrn.split(key)
    X = simulate_states(x0, A, Sigma, N, subkey).transpose((1, 0, 2))

    S = vmap(vmatmul, (0, 0))(B, X)

    # samples x time x space
    X = X.transpose((1, 0, 2))

    S = S.transpose((1, 0, 2))

    key, subkey = jrn.split(key)
    Y = MVN(S, Omega).sample(seed=subkey)

    return X, Y

# %% ../nbs/00_glssm.ipynb 11
from .kalman import kalman
def simulate_smoothed_FW1994(
    x_filt: Float[Array, "n+1 m"],
    Xi_filt: Float[Array, "n+1 m m"],
    Xi_pred: Float[Array, "n+1 m m"],
    A: Float[Array, "n m m"],
    N: int,
    key: PRNGKeyArray
) -> Float[Array, "N n+1 m"]:
    r"""Simulate from smoothing distribution $p(X_0, \dots, X_n|Y_0, \dots, Y_n)$

    Parameters
    ----------
    x_filt :
        filtered states $\hat X_{t\vert t}$
    Xi_filt :
        filtered covariance matrices $\Xi_{t\vert t}$, $t=0,\dots,n$
    Xi_pred :
        prediction covariance matrices $\Xi_{t + 1 \vert t}$, $t=0, \dots, n-1$
    A :
        transition matrices $A_t$, $t=0,\dots,n-1$
    N :
        number of samples
    key :
        the random states

    Returns
    -------
    X :
        an array of N samples from the smoothing distribution
    """

    key, subkey = jrn.split(key)
    X_n = MVN(x_filt[-1], Xi_filt[-1]).sample(N, subkey)

    def sample_backwards(carry, inputs):
        X_smooth_next, key = carry
        x_filt, Xi_filt, Xi_pred, A = inputs

        G = Xi_filt @ jnp.linalg.solve(Xi_pred, A).T

        cond_expectation = x_filt + vmatmul(G, X_smooth_next - (A @ x_filt)[None])
        cond_covariance = Xi_filt - G @ Xi_pred @ G.T

        key, subkey = jrn.split(key)
        new_samples = MVN(cond_expectation, cond_covariance).sample(seed=subkey)
        return (new_samples, key), new_samples

    key, subkey = jrn.split(key)
    _, X = scan(
        sample_backwards,
        (X_n, subkey),
        (x_filt[:-1], Xi_filt[:-1], Xi_pred[1:], A),
        reverse=True,
    )

    X_full = jnp.concatenate((X, X_n[None]))

    return X_full.transpose((1, 0, 2))


def FFBS(
    y: Float[Array, "n+1 m"],
    x0: Float[Array, "m"],
    Sigma: Float[Array, "n+1 m m"],
    Omega: Float[Array, "n+1 p p"],
    A: Float[Array, "n m m"],
    B: Float[Array, "n+1 p m"],
    N: int,
    key: PRNGKeyArray
) -> Float[Array, "N n+1 m"]:
    r"""The Forward-Filter Backwards-Sampling Algorithm

    From [@Fruhwirth-Schnatter1994Data].

    Parameters
    ----------
    y :
        Observations $y$
    x0 :
        initial mean $\mathbf E X_0$
    Sigma :
        innovation covariances $\Sigma_t$
    Omega :
        errors $\Omega_t$
    A :
        transition matrices $A_t$
    B :
        observation matrices $B_t$
    N : int
        number of samples
    key : PRNGKeyArray
        random state

    Returns
    -------
    X :
        an array of N samples from the smoothing distribution
    """
    x_filt, Xi_filt, _, Xi_pred = kalman(y, x0, Sigma, Omega, A, B)

    key, subkey = jrn.split(key)
    return simulate_smoothed_FW1994(x_filt, Xi_filt, Xi_pred, A, N, subkey)

# %% ../nbs/00_glssm.ipynb 16
def log_probs_x(
    x: Float[Array, "n+1 m"],  # the states
    x0: Float[Array, "m"],  # initial mean
    A: Float[Array, "n m m"],  # transition matrices
    Sigma: Float[Array, "n+1 m m"],  # innovation covariances
) -> Float[Array, "n+1"]:  # log probabilities $p(x_t | x_{t-1})$
    (m,) = x0.shape
    A_ext = jnp.concatenate((jnp.eye(m)[jnp.newaxis], A))
    x_prev = jnp.concatenate((x0[None], x[:-1]))
    x_pred = (A_ext @ x_prev[:, :, None])[:,:,0]
    return MVN(x_pred, Sigma).log_prob(x)


def log_probs_y(
    y: Float[Array, "n+1 p"],  # the observations
    x: Float[Array, "n+1 m"],  # the states
    B: Float[Array, "n+1 p m"],  # observation matrices
    Omega: Float[Array, "n+1 p p"],  # error covariances
) -> Float[Array, "n+1"]:  # log probabilities $p(y_t | x_t)$
    y_pred = (B @ x[:, :, None])[:, :, 0]
    return MVN(y_pred, Omega).log_prob(y)


def log_prob(
    x: Float[Array, "n+1 m"],
    y: Float[Array, "n+1 p"],
    x0: Float[Array, "m"],
    A: Float[Array, "n m m"],
    B: Float[Array, "n+1 p m"],
    Sigma: Float[Array, "n+1 m m"],
    Omega: Float[Array, "n+1 p p"],
):
    return jnp.sum(log_probs_x(x, x0, A, Sigma)) + jnp.sum(log_probs_y(y, x, B, Omega))
