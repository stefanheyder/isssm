# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/45_cross_entropy_method.ipynb.

# %% auto 0
__all__ = ['CEMProposal', 'simulate_cem', 'proposal_from_moments', 'log_pdf', 'posterior_markov_proposal', 'log_weight_cem',
           'cross_entropy_method']

# %% ../nbs/45_cross_entropy_method.ipynb 1
import jax.numpy as jnp
import jax.scipy as jsp
import jax.random as jrn
from .typing import MarkovProcessCholeskyComponents
from jax import vmap, jit
from .importance_sampling import ess_pct
from .pgssm import log_prob as log_prob_joint
import tensorflow_probability.substrates.jax.distributions as tfd
from jaxtyping import Float, Array, PRNGKeyArray
from typing import Tuple
from .importance_sampling import normalize_weights
from .util import converged
from jax.lax import while_loop, fori_loop, scan

# %% ../nbs/45_cross_entropy_method.ipynb 7
from .typing import PGSSM, Observations
from .util import mm_sim, mm_time_sim
import jax.scipy.linalg as jsla
from typing import NamedTuple
from .util import location_antithetic, scale_antithethic
from functools import partial

class CEMProposal(NamedTuple):
    mean: Float[Array, "n+1 m"]
    R: Float[Array, "n+1 m m"]
    J_tt: Float[Array, "n m m"] # lower triangular
    J_tp1t: Float[Array, "n m m"]

def simulate_cem(proposal: CEMProposal, N: int, key: PRNGKeyArray) -> Float[Array, "N n+1 m"]:
    np1, m = proposal.mean.shape
    key, subkey = jrn.split(key)
    u = MVN(loc=jnp.zeros(m), covariance_matrix=jnp.eye(m)).sample((N, np1), subkey)
    
    # transpose to have time in first dimension
    eps = mm_time_sim(proposal.R, u).transpose((1, 0, 2))
    lower_tri_solve = partial(jsla.solve_triangular, lower=True)
    def _iteration(carry, inputs):
        x_prev, = carry
        eps, J_tt, J_tp1t = inputs

        x_next = mm_sim(J_tp1t, vmap(lower_tri_solve, (None, 0))(J_tt, x_prev)) + eps

        return (x_next,), x_next

    J_tt_ext = jnp.concatenate([jnp.eye(m)[None], proposal.J_tt], axis=0)
    J_tp1t_ext = jnp.concatenate([jnp.eye(m)[None], proposal.J_tp1t], axis=0)
    _, x = scan(
        _iteration,
        (jnp.zeros((N, m)),),
        (eps, J_tt_ext, J_tp1t_ext)
    )

    mean = proposal.mean
    samples = x.transpose((1,0,2)) + proposal.mean

    l_samples = location_antithetic(samples, mean)
    s_samples = scale_antithethic(u, samples, mean)
    ls_samples = scale_antithethic(u, l_samples, mean)

    return jnp.concatenate((
        samples,
        l_samples,
        s_samples,
        ls_samples
    ), axis=0)

# %% ../nbs/45_cross_entropy_method.ipynb 8
from .util import degenerate_cholesky
import jax.scipy.linalg as jsla
def proposal_from_moments(
    mean: Float[Array, "n+1 m"],
    consecutive_covs: Float[Array, "n 2*m 2*m"]
) -> CEMProposal:
    np1,m = mean.shape
    
    chols = degenerate_cholesky(
        jnp.concatenate((
            jsla.block_diag(jnp.zeros((m, m)), consecutive_covs[0][:m, :m])[None],
            consecutive_covs
        ), axis=0)
    )

    J_tt = chols[1:, :m, :m]
    J_tp1t = chols[1:, m:, :m]
    Rs = chols[:, m:, m:]

    return CEMProposal(
        mean=mean,
        R=Rs,
        J_tt=J_tt,
        J_tp1t=J_tp1t
    )

# %% ../nbs/45_cross_entropy_method.ipynb 12
from .util import mm_time
def log_pdf(
    x: Float[Array, "n+1 m"],
    proposal: CEMProposal
):
    np1, m = x.shape

    lower_tri_solve = partial(jsla.solve_triangular, lower=True)
    centered = x - proposal.mean
    eps = centered[1:] - mm_time(proposal.J_tp1t, vmap(lower_tri_solve)(proposal.J_tt, centered[:-1])) 
    eps = jnp.concatenate((centered[:1], eps), axis=0)

    nu = vmap(lower_tri_solve)(proposal.R, eps)

    return MVN(jnp.zeros(m), jnp.eye(m)).log_prob(nu).sum() - jnp.log(vmap(jnp.diag)(proposal.R)).sum()

# %% ../nbs/45_cross_entropy_method.ipynb 16
from .typing import GLSSM
from .kalman import kalman, smoother
def _joint_cov(Xi_smooth_t, Xi_smooth_tp1, Xi_filt_t, Xi_pred_tp1, A_t):
    """Joint covariance of conditional Markov process"""
    off_diag = Xi_filt_t @ A_t.T @ jnp.linalg.pinv(Xi_pred_tp1) @ Xi_smooth_tp1 #jnp.linalg.solve(Xi_pred_tp1, Xi_smooth_tp1)
    return jnp.block([[Xi_smooth_t, off_diag], [off_diag.T, Xi_smooth_tp1]])


def posterior_markov_proposal(y: Observations, model: GLSSM) -> CEMProposal:
    filtered = kalman(y, model)
    _, Xi_filter, _, Xi_pred = filtered
    x_smooth, Xi_smooth = smoother(filtered, model.A)

    covs = vmap(_joint_cov)(
        Xi_smooth[:-1], Xi_smooth[1:], Xi_filter[:-1], Xi_pred[1:], model.A
    )

    return proposal_from_moments(x_smooth, covs)

# %% ../nbs/45_cross_entropy_method.ipynb 17
from functools import partial
from .typing import GLSSM


def log_weight_cem(
    x: Float[Array, "n+1 m"], y: Observations, model: PGSSM, proposal: CEMProposal
) -> Float:

    log_p = log_prob_joint(x, y, model)
    log_g = log_pdf(x, proposal)

    return log_p - log_g


def cross_entropy_method(
    model: PGSSM,
    y: Observations,
    N: int,
    key: PRNGKeyArray,
    n_iter: int,
) -> CEMProposal:
    key, subkey_crn = jrn.split(key)

    _, z, Omega = laplace_approximation(y, model, jnp.log(y + 1.0), n_iter)
    glssm_la = GLSSM(model.x0, model.A, model.Sigma, model.B, Omega)
    initial = posterior_markov_proposal(z, glssm_la)

    def _iteration(i, vals):
        proposal, _ = vals

        samples = simulate_cem(proposal, N, subkey_crn)

        _N, np1, m = samples.shape

        log_w = vmap(partial(log_weight_cem, y=y, model=model, proposal=proposal))(
            samples
        )
        w = normalize_weights(log_w)

        mean = jnp.sum(w[:, None, None] * samples, axis=0)
        x_consecutive = jnp.concatenate((samples[:, :-1], samples[:, 1:]), axis=-1)

        # ensure matrices (in 1D case jnp.cov reduces to (np1, m))
        consecutive_covs = vmap(partial(jnp.cov, aweights=w, rowvar=False), 1)(
            x_consecutive
        ).reshape((np1 - 1, 2 * m, 2 * m))

        return proposal_from_moments(mean, consecutive_covs), log_w

    final_proposal, log_w = fori_loop(
        0, n_iter, _iteration, (initial, jnp.empty(4 * N))
    )

    return final_proposal, log_w
