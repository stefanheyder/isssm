# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/45_cross_entropy_method.ipynb.

# %% auto 0
__all__ = ['ce_cholesky_block', 'ce_cholesky_last', 'cholesky_components', 'log_prob', 'ce_log_weights']

# %% ../nbs/45_cross_entropy_method.ipynb 1
import jax.numpy as jnp
import jax.scipy as jsp
import jax.random as jrn
from jax import vmap
from .importance_sampling import ess_pct
from .lcssm import log_prob as log_prob_joint
import tensorflow_probability.substrates.jax.distributions as tfd
from jaxtyping import Float, Array, PRNGKeyArray
from typing import Tuple

# %% ../nbs/45_cross_entropy_method.ipynb 8
def ce_cholesky_block(
    x: Float[Array, "N m"],  # samples of $X_t$
    x_next: Float[Array, "N m"],  # samples of $X_{t+1}$
    weights: Float[Array, "N"],  # $w$, need not be normalized
) -> Float[Array, "2*m m"]:  # Cholesky factor
    """Calculate the columns and section of rows of the Cholesky factor of $P$ corresponding to $X_t$, $t < n$."""
    _, m = x.shape
    joint_x = jnp.concatenate([x, x_next], axis=1)
    weights = weights / weights.sum()

    joint_mean = jnp.sum(joint_x * weights[:, None], axis=0)
    cov = jnp.atleast_2d(
        jnp.sum(
            (joint_x[:, :, None] @ joint_x[:, None, :]) * weights[:, None, None], axis=0
        )
        - joint_mean[:, None] @ joint_mean[None, :]
    )

    L = jnp.zeros((2 * m, m))
    for i in range(m):
        sub_cov = cov[i:, i:]
        v = jnp.linalg.solve(sub_cov, jnp.eye(2 * m - i)[0])
        lam = jnp.sqrt(v[0])
        L = L.at[i:, i].set(v / lam)

    return L


def ce_cholesky_last(
    x: Float[Array, "N m"],  # samples of $X_n$
    weights: Float[Array, "N"],  # $w$, need not be normalized
) -> Float[Array, "m m"]:  # Cholesky factor
    """Calculate the Cholesky factor of $P$ corresponding to $X_n$."""
    _, m = x.shape
    L = jnp.zeros((m, m))
    weights = weights / weights.sum()

    mean = jnp.sum(x * weights[:, None], axis=0)

    cov = jnp.atleast_2d(
        jnp.sum((x[:, :, None] @ x[:, None, :]) * weights[:, None, None])
        - mean @ mean.T
    )

    for i in range(m):
        sub_cov = cov[i:, i:]
        v = jnp.linalg.solve(sub_cov, jnp.eye(m - i)[0])
        lam = jnp.sqrt(v[0])
        L = L.at[i:, i].set(v / lam)
    return L


def cholesky_components(
    samples: Float[Array, "N n m"],  # samples of $X_1, \ldots, X_n$
    weights: Float[Array, "N"],  # $w$, need not be normalized
) -> Tuple[
    Float[Array, "n m m"], Float[Array, "n m m"]
]:  # block diagonal and off-diagonal components
    """calculate all components of the Cholesky factor of $P$"""
    current = samples[:, :-1]
    next = samples[:, 1:]

    diag, off_diag = jnp.split(
        vmap(ce_cholesky_block, (1, 1, None))(current, next, weights), 2, 1
    )
    last_diag = ce_cholesky_last(samples[:, -1], weights)
    full_diag = jnp.concatenate([diag, last_diag[None, :]], axis=0)

    return full_diag, off_diag

# %% ../nbs/45_cross_entropy_method.ipynb 17
def log_prob(
    x: Float[Array, "n+1 m"], # the location at which to evaluate the likelihood
    full_diag: Float[Array, "n+1 m m"],# block diagonals of $L$
    off_diag: Float[Array, "n m m"], # off-diagonals of $L$
    mean: Float[Array, "n+1 m"], # mean of the process
) -> Float: # log-likelihood
    np1, m = x.shape

    # append zeros to have the same shape as full_diag
    extended_off_diag = jnp.concatenate([off_diag, jnp.zeros((m, m))[None]], axis=0)
    centered = x - mean

    # L is triangular
    logdet = 2 * jnp.sum(jnp.log(vmap(jnp.diag)(full_diag)))

    # exploit sparsity of L
    extended_centered = jnp.concatenate([centered, jnp.zeros((1, m))], axis=0)
    Lt_x = (
        full_diag.transpose((0, 2, 1)) @ extended_centered[:-1,:,None]
        + extended_off_diag.transpose((0, 2, 1)) @ extended_centered[1:,:,None]
    )[:,:,0]
    l2_norm = jnp.sum(Lt_x ** 2)

    return -np1 * m / 2 * jnp.log(2 * jnp.pi) - 1 / 2 * logdet - 1 / 2 * l2_norm

# %% ../nbs/45_cross_entropy_method.ipynb 24
def ce_log_weights(
    x: Float[Array, "n+1 m"], # the sample
    y: Float[Array, "n+1 p"], # the observations
    full_diag: Float[Array, "n+1 m m"],# block diagonals of $L$
    off_diag: Float[Array, "n m m"], # off-diagonals of $L$
    mean: Float[Array, "n+1 m"], # mean of the process
    x0: Float[Array, "m"], # initial state
    A: Float[Array, "n m m"], # transition matrix
    Sigma: Float[Array, "n+1 m m"], # covariance matrix
    B: Float[Array, "n+1 p m"], # observation matrix
    dist: tfd.Distribution, # distribution of the initial state
    xi: Float[Array, "n+1 m"], # initial state
) -> Float: # log-weights
    log_p = log_prob_joint(x, y, x0, A, Sigma, B, dist, xi)
    log_g = log_prob(x, full_diag, off_diag, mean)

    return log_p - log_g
