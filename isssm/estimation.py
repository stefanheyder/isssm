# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/60_maximum_likelihood_estimation.ipynb.

# %% auto 0
__all__ = ['vmm', 'gnll', 'gnll_full', 'mle_glssm', 'mle_glssm_ad', 'lcnll', 'initial_theta', 'mle_lcssm']

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 2
import jax.numpy as jnp
import jax.random as jrn
from jax import vmap
from jaxtyping import Float, Array, PRNGKeyArray
from .kalman import kalman
from jax import jit
from scipy.optimize import minimize as minimize_scipy
from .mode_estimation import mode_estimation as LA
from .modified_efficient_importance_sampling import modified_efficient_importance_sampling as MEIS
from .importance_sampling import normalize_weights
from .typing import GLSSM, PGSSM

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 6
from .util import MVN_degenerate as MVN
vmm = vmap(jnp.matmul, (0, 0))


@jit
def gnll(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    x_pred: Float[Array, "n+1 m"],  # predicted states $\hat X_{t+1\bar t}$
    Xi_pred: Float[Array, "n+1 m m"],  # predicted state covariances $\Xi_{t+1\bar t}$
    B: Float[Array, "n+1 p m"],  # state observation matrices $B_{t}$
    Omega: Float[Array, "n+1 p p"],  # observation covariances $\Omega_{t}$
) -> Float:  # gaussian negative log-likelihood
    """Gaussian negative log-likelihood"""
    y_pred = vmm(B, x_pred)
    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega

    return -MVN(y_pred, Psi_pred).log_prob(y).sum()

@jit
def gnll_full(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model: GLSSM
):
    filtered = kalman(y, model)
    return gnll(y, filtered.x_pred, filtered.Xi_pred, model.B, model.Omega)

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 9
from scipy.optimize import minimize as minimize_scipy
from jax.scipy.optimize import minimize as minimize_jax
from scipy.optimize import OptimizeResult
from jax.scipy.optimize import OptimizeResults

def mle_glssm(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model_fn,  # parameterize GLSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    options=None, # options for the optimizer
) -> OptimizeResult:  # result of MLE optimization
    """Maximum likelihood estimation for GLSSM"""
    @jit
    def f(theta: Float[Array, "k"]) -> Float:
        model = model_fn(theta, aux)
        return gnll_full(y, model)

    return minimize_scipy(f, theta0, method="BFGS", options=options)

def mle_glssm_ad(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model_fn,  # parameterize GLSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    options=None, # options for the optimizer
) -> OptimizeResults:  # result of MLE optimization
    """Maximum likelihood estimation for GLSSM using automatic differentiation"""
    
    def f(theta: Float[Array, "k"]) -> Float:
        model = model_fn(theta, aux)
        return gnll_full(y, model)

    return minimize_jax(f, theta0, method="BFGS", options=options)

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 16
from jax.scipy.special import logsumexp
from .importance_sampling import lcssm_importance_sampling
from .kalman import kalman
from .typing import GLSSM, PGSSM


def _lcnll(
    gnll: Float,  # surrogate gaussian negative log-likelihood
    unnormalized_log_weights: Float[
        Array, "N"
    ],  # unnormalized log-weights $\log w(X^i)$
) -> Float:  # the approximate negative log-likelihood
    """Internal Log-Concave Negative Log-Likelihood"""
    (N,) = unnormalized_log_weights.shape
    weights = normalize_weights(unnormalized_log_weights)
    return gnll - logsumexp(unnormalized_log_weights) + jnp.log(N) #- (jnp.var(weights) / (2 * N * jnp.mean(weights) ** 2))


def lcnll(
    y: Float[Array, "n+1 p"],  # observations
    model: PGSSM,  # the model
    z: Float[Array, "n+1 p"],  # synthetic observations
    Omega: Float[Array, "n+1 p p"],  # covariance of synthetic observations
    N: int,  # number of samples
    key: PRNGKeyArray,  # random key
) -> Float:  # the approximate negative log-likelihood
    """Log-Concave Negative Log-Likelihood"""

    key, subkey = jrn.split(key)

    _, log_weights = lcssm_importance_sampling(y, model, z, Omega, N, subkey)

    return _lcnll(
        gnll_full(z, GLSSM(model.x0, model.A, model.Sigma, model.B, Omega)), log_weights
    )

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 19
from .importance_sampling import log_weights

def initial_theta(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model_fn,  # parameterized LCSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    n_iter_me: int, # number of mode estimation iterations
    s_init: Float[Array, "n+1 m"],  # initial signals for mode estimation
    options=None, # options for the optimizer
):
    """Initial value for Maximum Likelihood Estimation for Log Concave SSMs"""
    @jit
    def f(theta):
        model = model_fn(theta, aux)
        
        X, z, Omega = LA(y, model, s_init, n_iter_me)

        x0, A, Sigma, B, *_ = model
        glssm_la = GLSSM(x0, A, Sigma, B, Omega)
        signal = (B @ X[:,:,None])[:,:,0]
        _, _, x_pred, Xi_pred = kalman(z, glssm_la)

        negloglik = gnll(z, x_pred, Xi_pred, B, Omega) - log_weights(signal, y, model.dist, model.xi, z, Omega).sum()
        return negloglik
    
    result = minimize_scipy(f, theta0, method="BFGS", options=options)
    return result

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 23
def mle_lcssm(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model_fn,  # parameterized LCSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    n_iter_me: int, # number of mode estimation iterations
    s_init: Float[Array, "n+1 m"],  # initial signals for mode estimation
    N: int, # number of importance samples
    key: Array, # random key
    options=None, # options for the optimizer
) -> Float[Array, "k"]: # MLE
    """Maximum Likelihood Estimation for Log Concave SSMs"""

    @jit
    def f(theta, key):
        model = model_fn(theta, aux)
        
        _, z, Omega = LA(y, model, s_init, n_iter_me)

        #key, subkey = jrn.split(key)    
        #z, Omega = MEIS(y, model, z, Omega, n_iter_me, N, subkey)

        key, subkey = jrn.split(key)    
        return lcnll(y, model, z, Omega, N, subkey)
    
    key, subkey = jrn.split(key)
    result = minimize_scipy(f, theta0, method="BFGS", options=options, args=(subkey,))
    return result 
