# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/60_maximum_likelihood_estimation.ipynb.

# %% auto 0
__all__ = ['vmm', 'gnll', 'mle_glssm', 'mle_glssm_ad', 'lcnll', 'mle_lcssm']

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 2
import jax.numpy as jnp
import jax.random as jrn
from jax import vmap
from .kalman import kalman

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 5
from jaxtyping import Float, Array

vmm = vmap(jnp.matmul, (0, 0))


def gnll(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    x_pred: Float[Array, "n+1 m"],  # predicted states $\hat X_{t+1\bar t}$
    Xi_pred: Float[Array, "n+1 m m"],  # predicted state covariances $\Xi_{t+1\bar t}$
    B: Float[Array, "n+1 p m"],  # state observation matrices $B_{t}$
    Omega: Float[Array, "n+1 p p"],  # observation covariances $\Omega_{t}$
) -> Float:  # gaussian negative log-likelihood
    """Gaussian negative log-likelihood"""
    y_pred = vmm(B, x_pred)
    Psi_pred = vmm(vmm(B, Xi_pred), jnp.transpose(B, (0, 2, 1))) + Omega

    return -tfd.MultivariateNormalFullCovariance(y_pred, Psi_pred).log_prob(y).sum()

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 8
from scipy.optimize import minimize as minimize_scipy
from jax.scipy.optimize import minimize as minimize_jax
from scipy.optimize import OptimizeResult
from jax.scipy.optimize import OptimizeResults

def mle_glssm(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model,  # parameterize GLSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    options=None, # options for the optimizer
) -> OptimizeResult:  # result of MLE optimization
    """Maximum likelihood estimation for GLSSM"""
    def f(theta: Float[Array, "k"]) -> Float:
        x0, A, B, Sigma, Omega = model(theta, aux)
        _, _, x_pred, Xi_pred = kalman(y, x0, Sigma, Omega, A, B)
        return gnll(y, x_pred, Xi_pred, B, Omega)

    return minimize_scipy(f, theta0, method="Nelder-Mead", options=options)

def mle_glssm_ad(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model,  # parameterize GLSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    options=None, # options for the optimizer
) -> OptimizeResults:  # result of MLE optimization
    """Maximum likelihood estimation for GLSSM using automatic differentiation"""
    
    def f(theta: Float[Array, "k"]) -> Float:
        x0, A, B, Sigma, Omega = model(theta, aux)
        _, _, x_pred, Xi_pred = kalman(y, x0, Sigma, Omega, A, B)
        return gnll(y, x_pred, Xi_pred, B, Omega)

    return minimize_jax(f, theta0, method="BFGS", options=options)

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 14
from jax.scipy.special import logsumexp
from .importance_sampling import lcssm_importance_sampling
from .kalman import kalman


def _lcnll(
    gnll: Float,  # surrogate gaussian negative log-likelihood
    unnormalized_log_weights: Float[
        Array, "N"
    ],  # unnormalized log-weights $\log w(X^i)$
) -> Float:  # the approximate negative log-likelihood
    """Internal Log-Concave Negative Log-Likelihood"""
    (N,) = unnormalized_log_weights.shape
    return gnll - logsumexp(unnormalized_log_weights) + jnp.log(N)


def lcnll(
    y: Float[Array, "n+1 p"],  # observations
    x0: Float[Array, "m"],  # initial state
    A: Float[Array, "n m m"],  # state transition matrices
    Sigma: Float[Array, "n+1 m m"],  # innovation covariance matrices
    B: Float[Array, "n+1 p m"],  # observation matrices
    xi_fun,  #
    dist,  # distribution of observations
    z: Float[Array, "n+1 p"],  # synthetic observations
    Omega: Float[Array, "n+1 p p"],  # covariance of synthetic observations
    N: int,  # number of samples
    key: jrn.KeyArray,  # random key
) -> Float:  # the approximate negative log-likelihood
    """Log-Concave Negative Log-Likelihood"""

    key, subkey = jrn.split(key)

    _, log_weights = lcssm_importance_sampling(
        y, x0, A, Sigma, B, xi_fun, dist, z, Omega, N, subkey
    )

    _, _, x_pred, Xi_pred = kalman(z, x0, Sigma, Omega, A, B)

    return _lcnll(gnll(y, x_pred, Xi_pred, B, Omega), log_weights)

# %% ../nbs/60_maximum_likelihood_estimation.ipynb 17
from scipy.optimize import minimize as minimize_scipy
from .mode_estimation import mode_estimation
from .importance_sampling import normalize_weights
from jax import grad
def mle_lcssm(
    y: Float[Array, "n+1 p"],  # observations $y_t$
    model,  # parameterized LCSSM
    theta0: Float[Array, "k"],  # initial parameter guess
    aux,  # auxiliary data for the model
    n_iter_me: int, # number of mode estimation iterations
    s_init: Float[Array, "n+1 m"],  # initial signals for mode estimation
    N: int, # number of importance samples
    key: jrn.KeyArray, # random key
    options=None, # options for the optimizer
) -> Float[Array, "k"]: # MLE
    """Maximum Likelihood Estimation for Log Concave SSMs"""

    key, subkey = jrn.split(key)
    def model_log_prob(theta, X):
        x0, A, Sigma, B, xi_fun, dist = model(theta, aux)
        n, _, _ = A.shape

        signal = (B @ X[:,:,None])[:,:,0]
        return dist(vmap(xi_fun)(jnp.arange(n+1), signal)).log_prob(y).sum()

    d_model_log_prob = grad(model_log_prob, argnums=0)
    vd_model_log_prob = vmap(d_model_log_prob, (None, 0))

    def f(theta):
        x0, A, Sigma, B, xi_fun, dist = model(theta, aux)
        
        _, z, Omega = mode_estimation(y, x0, A, Sigma, B, xi_fun, dist, s_init, n_iter_me)

        samples, log_weights = lcssm_importance_sampling(
            y, x0, A, Sigma, B, xi_fun, dist, z, Omega, N, subkey
        )

        _, _, x_pred, Xi_pred = kalman(z, x0, Sigma, Omega, A, B)

        negloglik = _lcnll(gnll(y, x_pred, Xi_pred, B, Omega), log_weights)
        norm_weights = normalize_weights(log_weights)
        gradient = -(vd_model_log_prob(theta, samples) * norm_weights[:, None]).sum(axis=0)
        return negloglik, gradient
    
    # Nelder-Mead does not use gradients
    return minimize_scipy(f, theta0, method="BFGS", options=options, jac=True)
