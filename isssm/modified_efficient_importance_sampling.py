# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/50_modified_efficient_importance_sampling.ipynb.

# %% auto 0
__all__ = ['modified_efficient_importance_sampling']

# %% ../nbs/50_modified_efficient_importance_sampling.ipynb 4
import jax.numpy as jnp
import jax.random as jrn
from jaxtyping import Float, Array, PRNGKeyArray
from jax import vmap
from .optim import converged
from .importance_sampling import log_weights_t, normalize_weights
from functools import partial
from jax.lax import while_loop
from .kalman import kalman
from jax.lax import scan
from tensorflow_probability.substrates.jax.distributions import (
    MultivariateNormalFullCovariance as MVN
)

from .glssm import vmatmul

def modified_efficient_importance_sampling(
    y: Float[Array, "n+1 p"], # observations
    x0: Float[Array, "m"], # initial state
    A: Float[Array, "n m m"], # state transition matrices
    Sigma: Float[Array, "n+1 m m"], # state noise covariance matrices
    B: Float[Array, "n+1 p m"],  # observation matrices
    xi: Float[Array, "n+1 p"], # observation parameters
    dist, # observation distribution
    z_init: Float[Array, "n+1 p"], # initial z estimate
    Omega_init: Float[Array, "n+1 p p"], # initial Omega estimate
    n_iter: int, # number of iterations
    N: int, # number of samples
    key: PRNGKeyArray, # random key
    eps: Float = 1e-5, # convergence threshold
):
    np1, p = y.shape
    n = np1 - 1

    lw_t = vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega))

    vB = vmap(partial(vmap(jnp.matmul), B))
    v_norm_w = vmap(normalize_weights)

    def _break(val):
        a, i, z, Omega, z_old, Omega_old, key = val

        z_converged = converged(z, z_old, eps)
        Omega_converged = converged(Omega, Omega_old, eps)
        iteration_limit_reached = i >= n_iter

        return jnp.logical_or(
            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached
        )

    def _iteration(val):
        a, i, z, Omega, _, _, initial_key = val

        x_filt, Xi_filt, x_pred, Xi_pred = kalman(z, x0, Sigma, Omega, A, B)

        key, subkey = jrn.split(initial_key)
        last_samples = MVN(x_filt[-1], Xi_filt[-1]).sample(N, seed=subkey)

        def optimal_parameters(signal: Float[Array, "N p"], weights: Float[Array, "N"], log_p: Float[Array, "N"]):
            ones = jnp.ones_like(weights)[:,None]
            w_inner_prod = lambda a, b: jnp.einsum('i,ij,ik->jk',weights, a, b)
            
            X_T_W_X = jnp.block([
                [w_inner_prod(ones, ones), w_inner_prod(ones, signal), w_inner_prod(ones, -.5 * signal**2)],
                [w_inner_prod(signal, ones), w_inner_prod(signal, signal), w_inner_prod(signal, -.5 * signal**2)],
                [w_inner_prod(-.5 * signal**2, ones), w_inner_prod(-.5 * signal**2, signal), w_inner_prod(-.5 * signal**2, -.5 * signal**2)]
            ])
            X_T_W_y = jnp.concatenate([
                w_inner_prod(ones, log_p[:,None]), w_inner_prod(signal, log_p[:, None]), w_inner_prod(-.5 * signal**2, log_p[:, None])
            ])
            
            return jnp.linalg.solve(X_T_W_X, X_T_W_y[:,0])

        def eis_parameters(B_t, xi_t, y_t, z_t, Omega_t, samples):
            signal_t = vmatmul(B_t, samples)
            log_p_t = dist(signal_t, xi_t).log_prob(y_t).sum(axis=-1)
            log_w_t = vmap(lambda s_t: log_weights_t(s_t, y_t, xi_t, dist, z_t, Omega_t))(signal_t)
            w_t_norm = normalize_weights(log_w_t)

            beta_t = optimal_parameters(signal_t, w_t_norm, log_p_t)
            return beta_t

        def sample_and_estimate(carry, inputs):
            previous_samples, key = carry
            x_filt_t, Xi_filt_t, Xi_pred_t, A_t, B_t, xi_t, y_t, z_t, Omega_t = inputs

            # sampling
            G_t = Xi_filt_t @ jnp.linalg.solve(Xi_pred_t, A_t).T

            cond_expectation = x_filt_t + vmatmul(G_t, previous_samples - (A_t @ x_filt_t)[None])
            cond_covariance = Xi_filt_t - G_t @ Xi_pred_t @ G_t.T

            key, subkey = jrn.split(key)
            new_samples = MVN(cond_expectation, cond_covariance).sample(seed=subkey)

            # estimation 
            beta_t = eis_parameters(B_t, xi_t, y_t, z_t, Omega_t, new_samples)

            return (new_samples, key), beta_t

        wls_estimate_n = eis_parameters(B[-1], xi[-1], y[-1], z[-1], Omega[-1], last_samples)
            
        key, subkey = jrn.split(key)
        _, wls_estimate = scan(
            sample_and_estimate,
            (last_samples, subkey),
            (x_filt[:-1], Xi_filt[:-1], Xi_pred[1:], A, B[:-1], xi[:-1], y[:-1], z[:-1], Omega[:-1]),
            reverse=True
        )
        wls_estimate = jnp.concatenate([wls_estimate, wls_estimate_n[None]])

        a = wls_estimate[:, 0]
        b = wls_estimate[:, 1 : (p + 1)]
        c = wls_estimate[:, (p + 1) :]

        z_new = b / c
        Omega_new = vmap(jnp.diag)(1 / c)


        return a, i+1, z_new, Omega_new, z, Omega, initial_key

    key, subkey = jrn.split(key)
    init = _iteration(
        (jnp.zeros(np1), 0, z_init, Omega_init, None, None, subkey)
    )

    _keep_going = lambda *args: jnp.logical_not(_break(*args))

    _, n_iters, z, Omega, _, _, _ = while_loop(
        _keep_going, _iteration, init
    )


    return z, Omega
