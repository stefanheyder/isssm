# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/50_modified_efficient_importance_sampling.ipynb.

# %% auto 0
__all__ = ['optimal_parameters', 'modified_efficient_importance_sampling_old', 'modified_efficient_importance_sampling']

# %% ../nbs/50_modified_efficient_importance_sampling.ipynb 4
import jax.numpy as jnp
import jax.random as jrn
from jaxtyping import Float, Array, PRNGKeyArray
from jax import vmap, jit
from .util import converged
from .importance_sampling import log_weights_t, normalize_weights
from functools import partial
from jax.lax import while_loop
from .kalman import kalman, simulation_smoother
from jax.lax import scan
from .util import MVN_degenerate as MVN, mm_sim

from .glssm import mm_sim
from .typing import GLSSM, PGSSM

@jit
def optimal_parameters(signal: Float[Array, "N p"], weights: Float[Array, "N"], log_p: Float[Array, "N"]):
    ones = jnp.ones_like(weights)[:,None]
    w_inner_prod = lambda a, b: jnp.einsum('i,ij,ik->jk',weights, a, b)
    
    X_T_W_X = jnp.block([
        [w_inner_prod(ones, ones), w_inner_prod(ones, signal), w_inner_prod(ones, -.5 * signal**2)],
        [w_inner_prod(signal, ones), w_inner_prod(signal, signal), w_inner_prod(signal, -.5 * signal**2)],
        [w_inner_prod(-.5 * signal**2, ones), w_inner_prod(-.5 * signal**2, signal), w_inner_prod(-.5 * signal**2, -.5 * signal**2)]
    ])
    X_T_W_y = jnp.concatenate([
        w_inner_prod(ones, log_p[:,None]), w_inner_prod(signal, log_p[:, None]), w_inner_prod(-.5 * signal**2, log_p[:, None])
    ])
    
    beta = jnp.linalg.solve(X_T_W_X, X_T_W_y[:,0])
    return beta 

def modified_efficient_importance_sampling_old(
    y: Float[Array, "n+1 p"], # observations
    model: PGSSM, # model
    z_init: Float[Array, "n+1 p"], # initial z estimate
    Omega_init: Float[Array, "n+1 p p"], # initial Omega estimate
    n_iter: int, # number of iterations
    N: int, # number of samples
    key: PRNGKeyArray, # random key
    eps: Float = 1e-5, # convergence threshold
):
    np1, p = y.shape
    x0, A, Sigma, B, dist, xi = model
    n = np1 - 1

    v_norm_w = vmap(normalize_weights)
    lw_t = vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega))

    vB = vmap(partial(vmap(jnp.matmul), B))

    def _break(val):
        a, i, z, Omega, z_old, Omega_old, key = val

        z_converged = converged(z, z_old, eps)
        Omega_converged = converged(Omega, Omega_old, eps)
        iteration_limit_reached = i >= n_iter

        return jnp.logical_or(
            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached
        )

    def _iteration(val):
        a, i, z, Omega, _, _, initial_key = val

        x_filt, Xi_filt, x_pred, Xi_pred = kalman(z, GLSSM(x0, A, Sigma, B, Omega))

        key, subkey = jrn.split(initial_key)
        last_samples = MVN(x_filt[-1], Xi_filt[-1]).sample(N, seed=subkey)


        def eis_parameters(B_t, xi_t, y_t, z_t, Omega_t, samples):
            signal_t = mm_sim(B_t, samples)
            log_p_t = dist(signal_t, xi_t).log_prob(y_t).sum(axis=-1)
            log_w_t = vmap(lambda s_t: log_weights_t(s_t, y_t, xi_t, dist, z_t, Omega_t))(signal_t)
            w_t_norm = normalize_weights(log_w_t)

            beta_t = optimal_parameters(signal_t, w_t_norm, log_p_t)
            return beta_t

        def sample_and_estimate(carry, inputs):
            previous_samples, key = carry
            x_filt_t, Xi_filt_t, Xi_pred_t, A_t, B_t, xi_t, y_t, z_t, Omega_t = inputs

            # sampling
            G_t = Xi_filt_t @ A_t.T @ jnp.linalg.pinv(Xi_pred_t)

            cond_expectation = x_filt_t + mm_sim(G_t, previous_samples - (A_t @ x_filt_t)[None])
            cond_covariance = Xi_filt_t - G_t @ Xi_pred_t @ G_t.T

            key, subkey = jrn.split(key)
            new_samples = MVN(cond_expectation, cond_covariance).sample(seed=subkey)

            # estimation 
            beta_t = eis_parameters(B_t, xi_t, y_t, z_t, Omega_t, new_samples)

            return (new_samples, key), beta_t

        wls_estimate_n = eis_parameters(B[-1], xi[-1], y[-1], z[-1], Omega[-1], last_samples)
            
        key, subkey = jrn.split(key)
        _, wls_estimate = scan(
            sample_and_estimate,
            (last_samples, subkey),
            (x_filt[:-1], Xi_filt[:-1], Xi_pred[1:], A, B[:-1], xi[:-1], y[:-1], z[:-1], Omega[:-1]),
            reverse=True
        )
        wls_estimate = jnp.concatenate([wls_estimate, wls_estimate_n[None]])

        a = wls_estimate[:, 0]
        b = wls_estimate[:, 1 : (p + 1)]
        c = wls_estimate[:, (p + 1) :]

        z_new = b / c
        Omega_new = vmap(jnp.diag)(1 / c)


        return a, i+1, z_new, Omega_new, z, Omega, initial_key

    key, subkey = jrn.split(key)
    init = _iteration(
        (jnp.zeros(np1), 0, z_init, Omega_init, None, None, subkey)
    )

    _keep_going = lambda *args: jnp.logical_not(_break(*args))

    _, n_iters, z, Omega, _, _, _ = while_loop(
        _keep_going, _iteration, init
    )

    return z, Omega


def modified_efficient_importance_sampling(
    y: Float[Array, "n+1 p"], # observations
    model: PGSSM, # model
    z_init: Float[Array, "n+1 p"], # initial z estimate
    Omega_init: Float[Array, "n+1 p p"], # initial Omega estimate
    n_iter: int, # number of iterations
    N: int, # number of samples
    key: PRNGKeyArray, # random key
    eps: Float = 1e-5, # convergence threshold
):
    z, Omega = z_init, Omega_init

    np1, p, m = model.B.shape

    key, crn_key = jrn.split(key)

    v_norm_w = vmap(normalize_weights)
    dist = model.dist
    lw_t = vmap(vmap(lambda s, y, xi, z, Omega: log_weights_t(s, y, xi, dist, z, Omega)), (0, None, None, None, None))

    def _break(val):
        i, z, Omega, z_old, Omega_old = val

        # in first iteration we don't have old values, converged is True for NaNs
        z_converged = jnp.logical_and(converged(z, z_old, eps), i > 0)
        Omega_converged = jnp.logical_and(converged(Omega, Omega_old, eps), i > 0)
        iteration_limit_reached = i >= n_iter

        return jnp.logical_or(
            jnp.logical_and(z_converged, Omega_converged), iteration_limit_reached
        )
    def _iteration(val):
        i, z, Omega, _, _ = val
        glssm_approx = GLSSM(model.x0, model.A, model.Sigma, model.B, Omega)
        sim_signal = simulation_smoother(glssm_approx, z, N, crn_key)

        log_weights = lw_t(sim_signal, y, model.xi, z, Omega)
        log_p = dist(sim_signal, model.xi).log_prob(y).sum(axis=-1)
        wls_estimate = vmap(optimal_parameters, (1,1,1), 0)(sim_signal, v_norm_w(log_weights), log_p)

        a = wls_estimate[:, 0]
        b = wls_estimate[:, 1 : (p + 1)]
        c = wls_estimate[:, (p + 1) :]

        z_new = b / c
        Omega_new = vmap(jnp.diag)(1 / c)

        return i + 1, z_new, Omega_new, z, Omega
    
    _keep_going = lambda *args: jnp.logical_not(_break(*args))

    n_iters, z, Omega, _, _ = while_loop(
        _keep_going, _iteration, 
        (0, z_init, Omega_init, jnp.empty_like(z_init), jnp.empty_like(Omega_init))
    )

    return z, Omega

