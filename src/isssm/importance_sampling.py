"""See also the corresponding [section in my thesis](https://stefanheyder.github.io/dissertation/thesis.pdf#nameddest=section.3.6)"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/40_importance_sampling.ipynb.

# %% auto 0
__all__ = ['normalize_weights', 'ess', 'ess_lw', 'ess_pct', 'mc_integration', 'prediction']

# %% ../../nbs/40_importance_sampling.ipynb 13
from jaxtyping import Array, Float


def normalize_weights(
    log_weights: Float[Array, "N"],  # log importance sampling weights
) -> Float[Array, "N"]:  # normalized importance sampling weights
    """Normalize importance sampling weights."""
    max_weight = jnp.max(log_weights)

    log_weights_corrected = log_weights - max_weight

    weights = jnp.exp(log_weights_corrected)

    return weights / weights.sum()

# %% ../../nbs/40_importance_sampling.ipynb 16
from jaxtyping import Array, Float


def ess(
    normalized_weights: Float[Array, "N"],  # normalized weights
) -> Float:  # the effective sample size
    """Compute the effective sample size of a set of normalized weights"""
    return 1 / (normalized_weights**2).sum()


def ess_lw(
    log_weights: Float[Array, "N"],  # the log weights
) -> Float:  # the effective sample size
    """Compute the effective sample size of a set of log weights"""
    return ess(normalize_weights(log_weights))


def ess_pct(
    log_weights: Float[Array, "N"],  # log weights
) -> Float:  # the effective sample size in percent, also called efficiency factor
    (N,) = log_weights.shape
    return ess_lw(log_weights) / N * 100

# %% ../../nbs/40_importance_sampling.ipynb 19
from jax import jit


@jit
def mc_integration(samples: Float[Array, "N ..."], log_weights: Float[Array, "N"]):
    return jnp.einsum("i...,i->...", samples, normalize_weights(log_weights))

# %% ../../nbs/40_importance_sampling.ipynb 24
from .kalman import to_signal_model


def prediction(
    f: callable,
    y,
    proposal: GLSSMProposal,
    model: PGSSM,
    N: int,
    key: PRNGKeyArray,
    probs: Float[Array, "k"],
    prediction_model=None,
):
    if prediction_model is None:
        prediction_model = model

    key, subkey = jrn.split(key)
    signal_samples, log_weights = pgssm_importance_sampling(
        y, model, proposal.z, proposal.Omega, N, subkey
    )
    N = signal_samples.shape[0]

    signal_model = to_signal_model(proposal)

    def state_sample(signal_sample, key):
        (x_sample,) = FFBS(signal_sample, signal_model, 1, key)
        return x_sample

    key, *subkeys = jrn.split(key, N + 1)
    subkeys = jnp.array(subkeys)
    x_samples = vmap(state_sample)(signal_samples, subkeys)
    s_samples = mm_time_sim(prediction_model.B, x_samples)

    key, subkey = jrn.split(key)
    y_prime_samples = prediction_model.dist(
        s_samples, prediction_model.xi[None]
    ).sample(seed=subkey)

    f_samples = vmap(f)(x_samples, signal_samples, y_prime_samples)

    mean_f = mc_integration(f_samples, log_weights)
    sd_f = jnp.sqrt(mc_integration(f_samples**2, log_weights) - mean_f**2)

    if f_samples.ndim == 3:
        percentiles = prediction_percentiles(
            f_samples, normalize_weights(log_weights), probs
        )
    elif f_samples.ndim == 2:
        percentiles = vmap(_prediction_percentiles, (1, None, None), 1)(
            f_samples, normalize_weights(log_weights), probs
        )
    elif f_samples.ndim == 1:
        percentiles = _prediction_percentiles(
            f_samples, normalize_weights(log_weights), probs
        )

    return mean_f, sd_f, percentiles
